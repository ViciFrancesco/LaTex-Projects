\documentclass[8pt]{extarticle}
\include{../style.tex}



\begin{document}
	\begin{formulario}

%PARAGRAFO 1
		\begin{tcenter}
\textbf{ALGORITMO}
		\end{tcenter}
Un algoritmo è una sequenza finita di operazioni, anche dette istruzioni, che riceve un input e restituisce un output al fine di risolvere un determinato problema.\\
\myRule

%PARAGRAFO 2
		\begin{tcenter}
\textbf{PROBLEMA DELL'ORDINAMENTO} 
		\end{tcenter}
Data in input una sequenza di numeri $n$ numeri $<a_1,a_2,...,a_n>$ si vuole ottenere in output una permutazione tale che:
		\begin{tcenter}
$<a_{1}^{'},a_{2}^{'},...,a_{n}^{'}>$ con $a_{1}^{'} \leq a_{2}^{'} \leq ... \leq a_{n}^{'} $
		\end{tcenter}
Il tipo di algoritmo usato per l'ordinamento dipende da fattori come: numero di elementi, ordinamento iniziale, tipo di memorizzazione usata, ecc...\\
\myRule

%PARAGRAFO 3
		\begin{tcenter}
\textbf{CORRETTEZZA DI UN ALGORITMO}
		\end{tcenter}
Un algoritmo viene detto "corretto" se ad ogni istanza di input termina con l'output giusto.\\
\textbf{Induzione Matematica}: Supponendo di voler dimostrare che la proprietà $P(n)$ vale per qualsiasi $n\in \mathbb{N}$. Definisco l'insieme universo $U={n\in\mathbb{N}\text{ t.c. vale } P(n)}$. Allora:\\

	\begin{descr}{1}
		\item[Passo Base]$\rightarrow$ dimostro che vale $P(1)$ o $P(0)$; 
		\item[Passo Induttivo]$\rightarrow$ suppongo che $P(n)$ valga per un generico $n$. Posso dunque concludere che $U$ coincide con $\mathbb{N}$.
	\end{descr}
\textbf{Invariante di Ciclo}: formulo un'affermazione che deve essere verificata in 3 diversi momenti:\\
	\begin{descr}{1}
		\item[Inizializzazione]$\rightarrow$ l'I.C. deve essere vera prima della prima iterazione del ciclo;
		\item[Conservazione]$\rightarrow$ l'I.C. deve essere vera prima della successiva iterazione del ciclo;
		\item[Conclusione]$\rightarrow$ l'I.C. al termine del ciclo deve fornire una condizione che permetta di verificare se l'output dell'algoritmo è corretto.
	\end{descr}
\myRule

%PARAGRAFO 4
	\begin{tcenter}
\textbf{ANALISI ASINTOTICA}
	\end{tcenter}
Il calcolo asintotico è usato per analizzare la complessità di un algoritmo ovvero per stimare quanto tale complessità aumenta all’aumentare della dimensione dell’input. Uso la funzione $T(n)$ in quanto la risorsa considerata è il tempo (misurato in operazioni elementari). Allora:
	\begin{descr}{0}
	
\item[Notazione $\bm{O}$] $\rightarrow$ $g(n)$ è un limite asintotico superiore per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{O(g(n))} =\{ f(n): \; \exists \, c\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq f(n)\leq c\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
		
\item[Notazione $\bm{o}$] $\rightarrow$ è definito come:
		\begin{tcenter}
$\bm{o(g(n))} =\{ f(n): \; \forall \, c\in\R ,\,\exists n_0>0\in\N $ t.c. $ \bm{0\leq f(n)< c\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
		
\item[Notazione $\bm{\Omega}$] $\rightarrow$ $g(n)$ è un limite asintotico inferiore per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{\Omega(g(n))} =\{ f(n): \; \exists \, c\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq c\cdot g(n)\leq f(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 		
		
\item[Notazione $\bm{\omega}$] $\rightarrow$ è definito come:
		\begin{tcenter}
$\bm{\omega(g(n))} =\{ f(n): \; \forall \, c\in\R ,\,\exists n_0>0\in\N $ t.c. $ \bm{0\leq c\cdot g(n)< f(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter}

\item[Notazione $\bm{\Theta}$] $\rightarrow$ $g(n)$ è un limite asintoticamente stretto per $f(n)$ ed è definito come:
		\begin{tcenter}
$\bm{\Theta(g(n))} =\{ f(n): \; \exists \, c_1,c_2\in\R^+ ,n_0\in\N $ t.c. $ \bm{0\leq c_1\cdot g(n)\leq f(n)\leq c_2\cdot g(n)} \;\; \forall\, n\geq n_0 \}$
		\end{tcenter} 
	\end{descr}
	
	\begin{descr}{1}
		\item[Teorema]: $f(n)=\Theta g(n) \text{ SSE } f(n)=O g(n) \text{ e } f(n)=\Omega g(n)$
		\item[Proprietà]: 
		\begin{descr}{0}
			\item[Transitiva]: $f(n)=\Theta(g(n))\text{ e }g(n)=\Theta(h(n)) \implies f(n)=\Theta(h(n))$ \quad (vale anche per $O,o,\Omega,\omega$);
			\item[Riflessiva]: $f(n)=\Theta(f(n))$ \quad (vale anche per $O,\Omega$);
			\item[Simmetria]: $f(n)=\Theta(g(n))\Leftrightarrow g(n)=\Theta(f(n))$;
			\item[Simmetria Trasposta]: $f(n)=O(g(n))\Leftrightarrow g(n)=\Omega(f(n))$ $f(n)=o(g(n))\Leftrightarrow g(n)=\omega(f(n))$
		\end{descr}
	\end{descr}
\myRule
%PARAGRAFO 5
	\begin{tcenter}
\textbf{CLASSI DI COMPLESSITA'}
	\end{tcenter}
Per categorizzare la complessità degli algoritmi faccio riferimento alla crescita di funzioni semplici. Le classi sono:\\
	\begin{descr}{0}
	\item[$\bm{O(1)}$] $\rightarrow$ complessità costante;
	\item[$\bm{O(kn)}$] con $k<1 \rightarrow$ complessità sottolineare (\underline{ES}: ricerca sequenziale);
	\item[$\bm{O(n)}$] $\rightarrow$ complessità lineare (\underline{ES}:ricerca sequenziale);
	\item[$\bm{O(n\cdot \ln n)}$] $\rightarrow$ (\underline{ES}: algoritmi di ordinamento ottimi);
	\item[$\bm{O(n^k)}$] con $k\geq 2\rightarrow$ (\underline{ES}: BubbleSort con $O(n^2)$); 
	\item[$\bm{O(k^n)}$] $\rightarrow$ complessità esponenziale;
	\end{descr}
\myRule

%PARAGRAFO 6
	\begin{tcenter}
\textbf{ALGORITMI ITERATIVI - INSERTION SORT}
	\end{tcenter}
È uno degli algoritmi iterativi più semplici. Ordina "in place" ed è efficiente per insiemi quasi ordinati. Lo pseudocodice è:
		\begin{codebox}
\Procname{$\proc{Insertion-Sort}(A)$}
\li \For $j \la 2$ \To $\attrib{A}{length}$ \Do
	\li $\id{key} \la A[j]$
	\li $i \la j-1$
	\li \While $i > 0$ \const{and} $A[i] > \id{key}$ \Do
		\li $A[i+1] \la A[i]$
		\li $i \la i-1$
	\End
	\li $A[i+1] \la \id{key}$
\End
		\end{codebox}
\textbf{Correttezza con I.C.} \\
"All'inizio di ogni iterazione del ciclo \textit{for}, il sottoarray $A[1,\dots,j-1]$ è ordinato ed è formato dagli stessi elementi che erano originariamente in $A[1,\dots,j-1]$, ma ordinati".\\
\textbf{Costo}
		\begin{descr}{1}
\item[Caso Ottimo] (Array Ordinato): $O(n)\rightarrow$ non si entra mai nel \textit{while} ma si esegue ogni volta il confronto, al contrario si esegue sempre il \textit{for}.
\item[Caso Peggiore] (Array Ordinato al Contrario): $O(n^2)\rightarrow$ si esegue il \textit{while} per il numero massimo di volte, il costo del \textit{for} è irrilevante rispetto a quello del \textit{while}.
\item[Caso Medio] (Array Disordinato): $O(n^2)\rightarrow$ il costo dipende dall'ordine ma statisticamente il \textit{while} verrà eseguito soltanto la metà delle volte rispetto al caso peggiore, il costo rimane quadratico. 
		\end{descr}
\myRule

%PARAGRAFO 7
		\begin{tcenter}
\textbf{ALGORITMI ITERATIVI - SELECTION SORT}
		\end{tcenter}
È un algoritmo molto semplice, opera "in place". Il suo obbiettivo è quello di trovare il minimo all'interno dell'array e inserirlo nella sequenza ordinata fino a qual momento. Lo pseudocodice è: \\		
		\begin{code}{SelectionSort(A)}
\li $n\la\attrib{A}{length}$
\li \For $j\la1 \To n-1$ \Do
	\li $\id{smallest}\la j$
	\li \For $i \la j+1 \To n$ \Do
		\li \If $A[i]<A[\id{smallest}]$ \Then
			\li $\id{smallest}\la i$
		\End
	\End
	\li \bf{exchange }$A[j]\sse A[smallest]$
		\end{code}				
\textbf{Correttezza con I.C.}
		\begin{descr}{1}
\item[Ciclo Interno] $\rightarrow$ "All'inizio della $i$-esima iterazione del \textit{for} interno, $A[min]$ è minore o uguale di ogni elemento di $A[j,\dots,i-1]$ cioè $\forall j\in [j,\dots,i-1]$ si ha che $A[min]\leq A[j]$". 
\item[Ciclo Esterno] $\rightarrow$ "All'inizio di ogni iterazione del ciclo \textit{for} esterno, il sottoarray $A[1,\dots,j-1]$ è ordinato e composto solo dagli elementi più piccoli dell'array $A$".
		\end{descr}
\bo{Costo}\\
In questo caso è sempre $O(n^2)$ perché, sia nel caso migliore sia nel caso peggiore, i due cicli \co{for} vengono comunque eseguiti. L'unica riga che può non essere eseguita è la 6, che è trascurabile al fine del calcolo del costo.\\
\myRule

%PARAGRAFO 8
		\begin{tcenter}
\bo{COMPLESSITÀ ALGORITMI RICORSIVI}
		\end{tcenter}
\bo{Equazione di Ricorrenza}\\
Si usa per stimare il tempo di esecuzione di algoritmi ricorsivi. Si fa rifermento alla divisione e combinazione di vari sottoproblemi. Infatti l'equazione è:
		\begin{tcenter}
$\mathbf{T(n)=		
		\begin{cases}
\Theta(1) \qquad \text{se } n\leq c\\
aT(\frac{n}{b})+D(n)+C(n) \qquad \text{altrimenti}
		\end{cases}}$
		\end{tcenter}  
Se $n\leq c$ con $c$ una costante, si ha il caso base; altrimenti si divide in $a$ sottoproblemi di dimensione $\frac{1}{b}$. Questo implica un costo $D(n)$ di divisione del problema e un costo $C(n)$ di combinazione dei sottoproblemi. I metodi di risoluzione di tale equazione sono:
		\begin{descr}{1} 
\item[Metodo di Sostituzione] $\rightarrow$ Ipotizzo una soluzione e la dimostro con un'induzione matematica:\\
			\begin{descr}{0}
\item[1.] indovino una soluzione e formulo un'ipotesi induttiva;
\item[2.] sostituisco nell'equazione di ricorrenza le epressioni $T(\cdot)$;
\item[3.] dimostro che è valida anche per il caso base.
			\end{descr}
La soluzione per una ricorrenza può essere:
			\begin{descr}{0}
\item[Esatta] $\rightarrow$ se la ricorrenza è formata da una funzione esatta allora anche la soluzione sarà sempre esatta;
\item[Asintotica] $\rightarrow$ mi riconduco ad una soluzione esatta, ipotizzo per prima cosa che la soluzione $T(n)$ sia per esempio un $O$ di qualche funzione e poi cerco le costanti che verificano l'ipotesi induttiva (quelle presenti nella definizione di $O$). 
			\end{descr}
\item[Metodo dell'Albero di Ricorsione] $\rightarrow$ È un metodo grafico per arrivare alla soluzione. L'albero deve sempre condurre ad un'equazione esatta. I nodi rappresentano i costi dei vari sottoproblemi e permette di aver un'idea del costo complessivo di tutte le esecuzioni dell'algoritmo. Si sommano i costi di tutti i vari sottolivelli per poi fare una somma complessiva di tali sottolivelli.
\item[Metodo dell'Esperto] $\rightarrow$ Sia $T(n)$ una funzione definita sui naturali dalla ricorrenza $T(n)=aT(\frac{n}{b})+f(n)$ con $a\geq 1, b>1$ costanti e $f(n)>0$ asintoticamente, allora $T(n)$ può essere limitata nei seguenti modi:
			\begin{descr}{0}
\item[caso 1]$\rightarrow$ se $f(n)=O(n^{\log_b a-\epsilon})$ per qualche costante $\epsilon>0$ allora: 
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a})}$
				\end{tcenter} 
\item[caso 2]$\rightarrow$ se $f(n)=\Theta(n^{\log_b a})$ allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a}\cdot \log_2 n)}$
				\end{tcenter}
\item[caso 3]$\rightarrow$ se $f(n)=\Omega(n^{log_b a+\epsilon})$ per qualche costante $\epsilon>0$ e $f(n)$ t.c. $a\;f(\frac{n}{b})\leq c\;f(n)$ per qualche costante $c<1$ e $\forall n\geq n_0$, allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(f(n))}$
				\end{tcenter}
\item[Condizione di Regolarità:]
Devo assicurarmi che quando scendo nell'albero $f(\cdot)$ diventa più piccola.
			\end{descr}
\item[Albero di Ricorsione + Esperto]$\rightarrow$ Combinando i metodi precedentemente descritti ottengo:
			\begin{descr}{0}
\item[caso 1]$\rightarrow$ se il costo cresce dalla radice alle foglie ("Costo Dominato dalle Foglie"), allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a})}$
				\end{tcenter}
\item[caso 2]$\rightarrow$ se il costo è circa lo stesso in ogni livello, allora:
				\begin{tcenter}
$\mathbf{T(n)=\Theta(n^{\log_b a}\cdot\log_2 n)}$
				\end{tcenter}
\item[caso 3]$\rightarrow$ se il costo decrementa dalla radice alle foglie ("Costo Dominato dalla Radice"), allora: 
				\begin{tcenter}
$\mathbf{T(n)=\Theta(f(n))}$
				\end{tcenter}
			\end{descr}
		\end{descr}
\myRule

%PARAGRAFO 9
		\begin{tcenter}
\bo{ALGORITMI RICORSIVI - MERGE SORT}
		\end{tcenter}
Divide ricorsivamente l'array da ordinare in due sottoarray di uguale lunghezza. Una volta arrivato alla lunghezza unitaria combina ricorsivamente i sottoarray ordinandoli. Lo pseudocodice è:
		\begin{code}{MergeSort(A,p,r)}
\li \IF{p<r} 
	\li $q\la \floor{(p+r)/2}$ 
	\li \proc{MergeSort}(A,p,q)
	\li \proc{MergeSort}(A,q+1,r)
	\li \proc{Merge}(A,p,q,r)
\End
		\end{code}		
		\begin{code}{Merge(A,p,q,r)}
\li $n_1 \la q-p+1$
\li $n_2 \la r-q$
\li \FOR{i \la 1 \To n-1} 
	\li $L[i] \la A[p+i-1]$
\End
\li \FOR{j \la 1 \To n_2} 
	\li $R[i] \la A[q+j]$
\End
\li $L[n_1+1]\la \infty$
\li $R[n_2+1]\la \infty$
\li $i \la 1$
\li $j \la 1$
\li \FOR{k \la p\To r}
	\li \IF{L[i]\leq R[j]}
		\li $A[k]\la L[i]$
		\li $i \la i+1$
	\li \Else
		\li $A[k]\la R[j]$
		\li $j \la j+1$
	\End
\End
		\end{code}		
\bo{Costo}\\
Ho che il costo di divisione è $D(n)=\Theta(1)$ mentre il costo di combinazione è $C(n)=\Theta(n)$ Inoltre ad ogni ricorrenza successiva l'algoritmo risolve $2$ problemi di dimensione $\frac{n}{2}$. Allora l'equazione di ricorrenza sarà: 
		\begin{tcenter}
$
T(n)=
		\begin{cases}
\Theta(1) \qquad\text{se }n=1 \\
2T(\frac{n}{2})+\Theta(1)+\Theta(n) \qquad\text{se }n>1
		\end{cases}
$
		\end{tcenter}
Poiché divido sempre il problema in 2 sottoproblemi di uguale dimensione è facile risolvere il problema con il metodo dell'albero di ricorrenza. Ogni livello ha costo $\Theta(n)$ ed essendo il numero di livelli $\log_2 n$ avrò che il costo del problema è:
		\begin{tcenter} 
$\mathbf{T(n)=}\Theta(n\cdot \log_2 n) + \Theta(n) + \Theta(1)=\mathbf{\Theta(n\cdot \log_2 n)}$
		\end{tcenter}
\myRule

%PARAGRAFO 10
		\begin{tcenter}
\bf{ALGORITMI RICORSIVI - QUICKSORT}
		\end{tcenter}
Divide ricorsivamente l'array da ordinare in due sottoarray grazie all'uso di due indici (ordina sul posto) grazie all'algoritmo "partition" per poi riordinarli e combinarli grazie all'algoritmo "quicksort". Lo pseudocodice è:

		\begin{code}{QuickSort(A,p,r)}
\li \IF{p<r}
	\li $q \la \proc{Partition}(A,p,r)$
	\li $\proc{QuickSort}(A,p,q-1)$
	\li $\proc{QuickSort}(A,q+1,r)$
\End
		\end{code}
		\begin{code}{Partition(A,p,r)}
\li $x\la A[r]$
\li $i\la p-1$
\li \FOR{j\la p \To r-1}
	\li \IF{A[j]\leq x}
		\li $i \la i+1$
		\li	 $\proc{Exchange }A[i] \sse A[j]$
	\End
\End
\li $\proc{Exchange } A[i+1]\sse A[r]$
\li \Return $i+1$	
		\end{code}
\bo{Costo}\\
		\begin{descr}{1}
\item[Partition] $\rightarrow$ ha costo fisso $\Theta(n)$.
\item[Quicksort] $\rightarrow$ il costo dipende dal partizionamento dei sottoarray. Infatti:
			\begin{descr}{0}
\item[Caso migliore] (sottoarray bilanciati) $\rightarrow$ ogni sottoarray ha dimensione $\leq \frac{n}{2}$ quindi il costo sarà $\Theta(\log_2 n)$;
\item[Caso peggiore] (sottoarray sbilanciati) $\rightarrow$ i due sottoarray sono di dimensione $0$ e $n-1$ quindi il costo complessivo sarà $\Theta(n^2)$;
\item[Caso Medio] $\rightarrow$ supponendo una suddivisione parzialmente sbilanciata (ad esempio 9 a 1) avremo un costo:
				\begin{tcenter}
$T(n)=T(\frac{9n}{10})+T(\frac{n}{10})+\Theta(n)=O(n\cdot \log_2 n)$
				\end{tcenter}
			\end{descr}
		\end{descr}
\myRule

%PARAGRAFO 11
	\begin{tcenter}
\bo{CENNI DI PROBABILITÀ}
	\end{tcenter}
Definisco $S$ come lo spazio degli eventi, ovvero come un insieme di eventi $E$ (esiti di esperimenti). Allora la probabilità $P$ che un evento si verifichi è:  
		\begin{tcenter}
		$\bm{P(E)=\frac{\text{casi favorevoli}}{\text{casi possibili}}}$
		\end{tcenter}
\bo{Proprietà:} 
	\begin{descr}{1}
\item[1.] $P(E)\geq 0 \quad \forall \text{ evento }E$;
\item[2.] $P(S)=1$;
\item[3.] $P(A \cup B)=P(A)+P(B)$ \quad se $A$ e $B$ sono mutuamente esclusivi;
\item[3.] $P(A \cup B)=P(A)+P(B)-P(A\cap B)$ \quad se $A$ e $B$ sono compatibili (cioè hanno eventi in comune).
	\end{descr}
\bo{Probabilità Composta:}
	\begin{descr}{1}
	
\item[Eventi Indipendenti]$\rightarrow$ Due eventi sono indipendenti se il realizzarsi di uno non influenza la probabilità di realizzarsi dell'altro. La probabilità che si realizzino entrambi è:
		\begin{tcenter}
$\bm{P(A\cap B)=P(A)\cdot P(B)}$
		\end{tcenter}
		
\item[Eventi Dipendenti]$\rightarrow$ Due eventi sono dipendenti se il realizzarsi di uno influenza la probabilità di realizzarsi dell'altro.  Indico con $P(B|A)$ la probabilità che si verifichi $B$ supponendo che si sia verificato $A$, allora la probabilità che si realizzino entrambi è:
		\begin{tcenter}
$\bm{P(A\cap B)= P(A)\cdot P(B|A)}$
		\end{tcenter}
		
	\end{descr}
\bo{Probabilità Condizionata:}
		\begin{tcenter}
$\bm{P(A|B)=\frac{P(A\cap B)}{P(B)}}$
		\end{tcenter}
		
		\begin{descr}{1}
\item[Teorema di Bayes] $\rightarrow$ Poiché $A\cap B=B\cap A$ ho che:
			\begin{tcenter}
$\bm{P(A\cap B)=P(B)P(A|B)=P(A)P(A|B)}$
			\end{tcenter}
		\end{descr}

	\begin{descr}{0}
\item[Variabile Aleatoria] $\rightarrow$ È l'insieme di tutti i possibili risultati che possono verificarsi in un esperimento.
\item[Distribuzione di Probabilità] $\rightarrow$ Elenca le probabilità di verificarsi di ogni valore della variabile aleatoria a cui si riferisce. La somma di tutte le probabilità di una distribuzione di probabilità è sempre pari a 1 (cioè $\sum P(X)=\sum_i P(x_i)=1$).
\item[Valore Atteso] $\rightarrow$ È il valore medio che mi aspetto da una lunga serie di osservazioni, ed è definito come: 
		\begin{tcenter}
$\bm{\mu=E[X]=\sum_{i=1}^n x_i\cdot P(x_i)}$
		\end{tcenter}
		\begin{descr}{0}
\item[Linearità]:
		\begin{tcenter}
$\bm{E[X+Y]=E[X]+E[Y]}$;\\
$g(x)=ax \implies \bm{E[aX]=aE[X]}$.
		\end{tcenter}
		\end{descr}
	\end{descr}
\myRule

%PARAGRAFO 12
		\begin{tcenter}
\bf{VARIABILI CASUALI INDICATRICI}
		\end{tcenter}
Dato uno spazio dei campioni $S$ e un evento $A$, si definisce variabile casuale indicatrice:
		\begin{tcenter}
$\bm{I\{A\}\text{ oppure }X_A=}		
		\begin{cases} 
1 \quad\text{se si verifica A;}\\
0 \quad\text{se non si verifica A}  
		\end{cases}$
		\end{tcenter}
		\begin{descr}{0}
\item[Lemma] $\rightarrow$ $E[X_a]=Pr\{ A \}$.
		\end{descr}
\myRule

%PARAGRAFO 13
		\begin{tcenter}
\bf{ALGORITMI RANDOMIZZATI}
		\end{tcenter}
Poiché non posso conoscere l'ordine in cui l'algoritmo riceverà gli input, non posso, allo stesso modo, conoscere la probabilità delle $n!$ possibili permutazioni. Uso l'algoritmo "random" per forzare l'ordine casuale. Lo pseudocodice è:
		\begin{code}{RandomizeInPlace(A)}
\li $n \la A.length$
\li \FOR{i \la 1 \To n}
	\li $\proc{excange } A[i]\sse A[\proc{Random}(i,n)]$
\End
		\end{code}
\bo{Costo:}\\
$\Theta(1)$ per ogni iterazione, quindi $\Theta(n)$ in totale.
\myRule

%PARAGRAFO 14
		\begin{tcenter}
\bo{ALGORITMI RANDOMIZZATI - QUICKSORT} 
		\end{tcenter}
Cerco di avvicinarmi il più possibile all'ipotesi di caso medio. Per farlo, randomizzo la scelta del pivot $q$ per l'algoritmo "partition", mentre l'algoritmo "quicksort" rimane uguale. Lo pseudocodice è: 
		\begin{code}{RandomizedPartition(A,p,r)}
\li $i\la \proc{Random}(p,r)$
\li \proc{Exchange }$A[r]\sse A[i]$
\li \Return $\proc{Partition} (A,p,r)$

		\end{code}
		\begin{code}{RandomizedQuickSort(A,p,r)}
\li \IF{p<r}
	\li $q\la$ \proc{RandomizedPartition}(A,p,r)
	\li \proc{RandomizedQuickSort}(A,p,q-1)
	\li \proc{RandomizedQuickSort}(A,q+1,r)
\End
		\end{code}
\bo{Costo}\\
Il tempo di esecuzione atteso di RandomizedQuickSort è $O(n\log_2n)$ in quanto statisticamente rispecchia il caso medio di QuickSort. 
\myRule

%PARAGRAFO 15
		\begin{tcenter}
\bo{ALBERO DI DECISIONE}
		\end{tcenter}
È un albero binario che rappresenta i possibili confronti fatti da un algoritmo su un input di una specifica dimensione. La radice rappresenta le prima due posizioni confrontate. Si procede a cascata analizzando tutti i possibili casi. Le foglie rappresenteranno infine tutte le possibili combinazioni degli input. 
		\begin{descr}{1}
\item[\underline{NOTA BENE}] La radice e i nodi intermedi sono strutturati come ("a":"b"). I confronti si effettuano sulla possibilità che sulla posizione "a" vi sia un valore $\leq />$ di quello in posizione "b". Non si fa dunque riferimento allo specifico valore ma solo alla relazione che lega la coppia di posizioni.
		\end{descr}
		\begin{descr}{0}
\item[Lemma] $\rightarrow$ Ogni albero binario di altezza $h$ ha al massimo $2^h$ foglie.
\item[Teorema] $\rightarrow$ Ogni albero di decisione che ordina $n$ elementi ha altezza $\Omega(n\log_2n)$ (cioè nel "caso migliore").
		\end{descr}
\myRule

%PARAGRAFO 16
		\begin{tcenter}
\bo{ORDINAMENTO IN TEMPO LINEARE}
		\end{tcenter}
Gli algoritmi visti fino ad ora non raggiungono un costo minore di $\Theta(n\log_2n)$, è possibile fare di meglio? In ogni caso si ha un costo di $\Omega(n)$ per esaminare tutti gli input.
\myRule

%PARAGRAFO 17
		\begin{tcenter}
\bo{ALGORITMI ITERATIVI - COUNTING SORT}
		\end{tcenter}
È un algoritmo di ordinamento che non si basa sui confronti, ma richiede due array di supporto per effettuare l'ordinamento. Inoltre può ordinare solo numeri naturali. Lo pseudocodice è: 
		\begin{code}{CountingSort(A,B,k)}
\li \FOR{i\la 0\To k}
	\li $C[i]\la 0$
\End
\li \FOR{j\la 1 \To \attrib{A}{length}}
	\li $C[A[j]]\la C[A[j]]+1$
\End
\li \FOR{i\la 1\To k}
	\li $C[i]\la C[i]+C[i-1]$
\End
\li \FOR{j\la \attrib{A}{length}\Downto 1}
	\li $B[C[A[j]]]\la A[j]$
	\li $C[A[j]]\la C[A[j]]-1$
\End
		\end{code}
\bo{Costo}\\
Il costo di CountingSort è $\Theta(n+k)$, che diventa $\Theta(n)$ se $k=O(n)$. Imponendo la dimensione massima di $k$ come condizione iniziale ottengo un ordinamento in tempo lineare.
		\begin{descr}{1}
\item[\underline{NOTA BENE}] CountingSort è stabile (se due valori sono uguali, il primo nell'array in input rimane primo nell'array di output).
		\end{descr} 
\myRule 

%PARAGRAFO 18
		\begin{tcenter}
\bo{ALGORITMI ITERATIVI - RADIX SORT}
		\end{tcenter}
È un algoritmo di ordinamento che usa il concetto controintuitivo di ordinare le singole cifre a partire dalla meno significativa. Necessita inoltre di riceve in ingresso in numero di cifre "d". Lo pseudocodice è: 
		\begin{code}{RadixSort(A,d)}
\li \FOR{i \la 1 \To d}
	\li "usa ordinamento stabile per ordinare l'array $A$ sulla cifra $i$"
\End
		\end{code}
\bo{Correttezza con I.C.}\\
"Prima della $i$-esima iterazione i numeri sono ordinati sulla base della $i-1$esima cifra meno significativa". \\
\bo{Costo}\\
Supponendo di usare CountingSort come algoritmo di ordinamento stabile, avrò che il costo complessivo è $\Theta(d(n+k))$ ed utilizzando la condizione $k=O(n)$ diventa $\Theta(d\cdot n)$. \\
\bo{Bilanciare Parole e Cifre} \\
Supponendo di dover ordinare $n$ parole di $b$ bit divise in cifre da $r$ bit, ho che il costo del CountingSort sarà $\bm{\Theta(\frac{b}{r}(n+2^r))}$. In particolare scegliendo $r\approx\log_2n$ ho che il costo (sostituendo) diventa $\bm{\Theta(\frac{b\cdot n}{\log_2n})}$ (meglio di $\Theta(n)$).
\myRule

%PARAGRAFO 19
		\begin{myParagraph}{HASHING}
Per la realizzazione di dizionari efficienti si usano le tabelle Hash che hanno tempo di ricerca atteso $O(1)$ mentre hanno $\Theta(n)$ nel caso peggiore. Dato un universo delle chiavi $U=\{ 0,1,\dots,m-1 \}$ esistono diversi modi per implementarle, ovvero:
			\begin{descr}{0}
				\item[Indirizzamento Diretto] $\rightarrow$ spesso il numero di chiavi memorizzate $K$ è molto minore dell'insieme delle chiavi $U$, si spreca quindi molto spazio nella tabella. Per risolvere questo problema si usano le funzioni Hash $h(k)$ memorizzando il valore in $T[h(k)]$ e non in $T[k]$. 
					\begin{descr}{0}
						\item[Metodo delle Divisioni] $\rightarrow$ una valida funzione Hash è quella data dal metodo delle divisioni, ovvero:
							\begin{tcenter}
								$\bm{h(k)=k \mod m}$
							\end{tcenter} 
Un buon criterio di scelta per la variabile $m$ è un numero primo non troppo vicino ad una potenza del 2 (se si usa una potenza del 2 si rischia di raggruppare i valori sulla base dei loro valori meno significativi).
						\item[Risoluzione delle Collisioni] $\rightarrow$ le funzioni Hash non sono iniettive quindi dovrò risolvere le collisioni che si creeranno, esistono 2 metodi: 
						\begin{descr}{0}
							\item[Concatenamento] $\rightarrow$ tutti gli elementi con lo stesso Hash sono memorizzati in una lista concatenata. 
							\begin{descr}{0}
								\item[Costo] $\rightarrow$ Definisco il fattore di caricamento $\alpha=\frac{n}{m}$, ovvero il numero medio di elementi in ogni lista collegata, con $n$ numero di elementi memorizzati e $m$ dimensione della tabella hash.
								\begin{descr}{0}
									\item[Caso Peggiore] $\rightarrow$ tutte le $n$ chiavi nello stesso slot, cioè si ha una singola lista di dimensione $n$:
									\begin{tcenter}
										$\Theta(1)+\Theta(n)=\Theta(n)$
									\end{tcenter}
									\item[Caso Medio] $\rightarrow$ Impongo come ipotesi di avere una funzione hash uniforme semplice (cioè per ogni elemento ognuno degli $m$ slot è ugualmente probabile come hash). Definisco $n_j$, dimensione della $j$-esima lista concatenata. Il suo valore atteso sarà $E[n_j]=\alpha=\frac{n}{m}$. Inoltre il calcolo della funzione hash avviene in $O(1)$.
									\begin{descr}{0}
										\item[Ricerca Senza Successo] $\rightarrow$ È necessario ricercare fino alla fine di ogni lista, quindi aggiungendo il costo del calcolo dell'hash avrò:
										\begin{tcenter}
											$\Theta(\alpha+1)$
										\end{tcenter}
										\item[Ricerca Con Successo] $\rightarrow$ Devo compiere un'analisi probabilistica sul numero di elementi esaminati $n_x+1$ ("$+1$" poiché esamino anche l'elemento che sto cercando $x$). Ottengo che il costo medio è:
										\begin{tcenter}
											$\Theta(2+\frac{\alpha}{2}-\frac{\alpha}{2n})=\Theta(1+\alpha)$  \\
											Se $n=O(m) \implies \alpha=O(1) \implies \Theta(1)$
										\end{tcenter}  
 									\end{descr}
 								\end{descr}								
							\end{descr}
							\item[Indirizzamento Aperto] $\rightarrow$ Memorizza tutte le chiavi nella tabella hash, ogni slot contiene una chiave o NIL. La funzione hash è nella forma $h(k,i)=(h'(k)+i)\mod m$ (è una delle possibili forme). Gli pseudocodici di inserimento e ricerca sono:
							\begin{code}{HashInsert(T,k)}
\li $i\la0$
\li \Repeat 
	\li	 $j\la h(k,i)$
		\li	 \IF{T[j]=\const{nil}}
			\li $T[j]\la k$
			\li \Return $j$
		\li \Else
			\li $i\la i+1$ 
		\End
\li \Until $i=m$
\li \Error "hash table overflow"
							\end{code}
							\begin{code}{HashSearch(T,k)}
\li $i\la0$
\li \Repeat 
	\li	 $j\la h(k,i)$
		\li	 \IF{T[j]=k}
			\li \Return $j$
		\End
		\li $i\la i+1$
\li \Until $T[j]=\const{nil } \const{or } i=m$
\li \Return \const{nil}
							\end{code}
							Per quanto riguarda la cancellazione si può sostituire il valore da cancellare con "DEL" che viene visto dalla ricerca come una chiave diversa da quella cercata e dall'inserimento come uno slot vuoto (il costo non dipenderà più da $\alpha$).
							\begin{descr}{0}
								\item[Hash Uniforme] $\rightarrow$ Ciascuna chiave ha la stessa probabilità che generi una delle $m!$ possibili sequenze di esplorazione. È difficile da implementare, quindi si approssima garantendo che la sequenza di esplorazione sia una delle $m!$ permutazioni. Per questo è necessario utilizzare funzioni hash ausiliari $h'():U\to\{ 0,1,...,m-1 \}$.
								\item[Esplorazione Lineare] $\rightarrow$ La funzione hash, data la chiave $k$ e il numero di esplorazione $i$, è nella forma:
									\begin{tcenter}
										$h(k,i)=(h'(k)+ci)\mod m$
									\end{tcenter}
Si hanno solo $m$ possibili sequenze e non $m!$. Si rischia di avere "\bo{clustering primario}" cioè lunghe sequenze di slot occupati.
								\item[Esplorazione Quadratica] $\rightarrow$ La funzione hash, data la chiave $k$ e il numero di esplorazione $i$, è nella forma:
									\begin{tcenter}
										$h(k,i)=(h'(k)+c_1i+c_2i^2)\mod m$
									\end{tcenter}
$c_1,c_2$ devono essere costanti e $\neq0$ e vanno scelti affinchè si abbia una permutazione completa (ES: $c_1=\frac{1}{2},c_2=\frac{1}{2}$).Si rischia di avere "\bo{clustering secondario}", cioè se $h'(k)=h'(k')$ con $k\neq k'$ allora le due chiavi avranno la stessa sequenza di esplorazione.
								\item[Doppio Hash] $\rightarrow$ Si va a definire il passo dell'esplorazione lineare usando una seconda funzione hash, la forma è:
									\begin{tcenter}
										$h(k,i)=(h_1'(k)+h_2'(k)i)\mod m$
									\end{tcenter}
$h_2'$ deve essere scelto affinché sia "primo" rispetto ad $m$ (non devono avere fattori comuni), per garantire che la sequenza di esplorazione una permutazione completa (ES: $h_1'(k)=k \mod m$, $h_2'(k)=1+(k \mod m')$). Si hanno $\Theta(m^2)$ diverse sequenze di esplorazione (molto meglio di $m$).
							\end{descr}
								\item[Metodo delle Moltiplicazioni] $\rightarrow$ Data una costante $A: 0<A<1$ la funzione hash è:
									\begin{tcenter}
										$\bm{h(k)=\floor{m(kA \mod 1)}}$
									\end{tcenter}
dove $kA \mod 1=$ "parte frazionaria di $kA$". Nonostante questo metodo sia più lento di quello delle divisioni, il valore di $m$ non è critico. La sua implementazione in base 2 è molto semplice infatti basta moltiplicare $k$ e $A$ per poi selezionare dal risultato i $p$ bit più significativi della parte frazionaria ($p=\log_2 m$). Anche la scelta di $A$ è importante (Knuth $\rightarrow A\approx \frac{\sqrt{5}-1}{2}$).
								\item[Hash Randomizzato] $\rightarrow$ Supponendo che la scelta delle chiavi sia affidata ad un avversario sleale, tutte le chiavi potrebbero essere inviate nello stesso slot. In questo caso sarebbe vantaggioso scegliere randomicamente la funzione la funzione hash da usare ogni volta che si inizia una nuova tabella (non ad ogni chiave inserita). 
								\item[Hash Universale] $\rightarrow$  La funzione hash ha la forma:
									\begin{tcenter}
										$h_{a,b}=((a\cdot k+b)\mod p)\mod m$
									\end{tcenter}
dove $p$ è un numero primo t.c. $\forall k \; 0\leq k\leq p-1$ e dove $a,b$ sono due numeri naturali t.c. $0\leq a\leq p-1 \text{ e }0\leq b\leq p$. In questo modo per ogni diverso $m$ si ha una famiglia di $p(p-1)$ diverse hash.
								\item[Hash Perfetto] $\rightarrow$ Si basa sull'ipotesi di avere un insieme di chiavi statico. Provo diverse funzioni hash fino a trovarne una che non genera collisioni. La complessità è $O(1)$ nel caso peggiore. Se $m$ è troppo grande o se devo provare la funzione hash troppe volte posso usare due livelli di hash:				
							\begin{descr}{0}
								\item[1° livello] $\rightarrow$ è un hash con concatenamento.
								\item[2° livello] $\rightarrow$ creo tabelle hash per ogni lista concatenata $T[i]$ del 1° livello di dimensione $n_i^2$ (dove $n_i$ è il numero di elementi nella $i$-esima lista concatenata). Sul 2° livello devo provare varie funzioni hash affinchè non si abbiano collisioni (la probabilità di averne è meno di metà della funzione hash perfetto ad un livello). 
							\end{descr}
						\end{descr}
					\end{descr}
			\end{descr}
		\end{myParagraph}

%PARAGARFO 20
		\begin{myParagraph}{ANALISI AMMORTIZZATA}
Si studia il tempo per eseguire una sequenza di operazioni (diverse) su una struttura dati. In questo caso non si fa una media su una distribuzione di input ma considero sempre il costo medio del caso peggiore per una sequenza di $n$ operazioni.
			\begin{descr}{0}
				\item[Tabelle Dinamiche] $\ra$ Data una tabella hash di dimensione $m$ con $n$ che varia, ho che se:
				\begin{descr}{0}
					\item[$\alpha=\frac{n}{m}=1$] $\ra$ rialloco la tabella, stavolta con dimensione $m'>m$, e copio gli oggetti;
					\item[$\alpha=\frac{n}{m}\leq0.5$] $\ra$ alloco la tabella, con dimensione $m'<m$, e copio gli oggetti.
				\end{descr}
È molto utile avere una tabella dinamica in quanto riduce lo spazio inutilizzato e permette di avere un costo ammortizzato per un'operazione di $O(1)$.
				\begin{code}{DynamicTableInsert(T,x)}
\li \IF{\attrib{T}{size}=0}
	\li "alloca tabella $\attrib{T}{size}$ con 1 elemento"
	\li $\attrib{T}{size} \la 1$
\End
\li \IF{\attrib{T}{num}=\attrib{T}{size}}
	\li "alloca NT (new table) con $2\cdot \attrib{T}{size}$ elementi"
	\li "inserisci gli elementi di $\attrib{T}{table}$ in NT"
	\li \func{free} $\attrib{T}{table}$
	\li $\attrib{T}{table}\la$ NT
	\li $\attrib{T}{size}\la 2\cdot\attrib{T}{size}$
\End
\li "inserisci $x$ in $\attrib{T}{table}$"
\li $\attrib{T}{num}\la \attrib{T}{num}+1$
				\end{code}
			\end{descr}
\bo{Costo}\\
Supponendo di compiere solo inserimenti avrò che costo della $i$-esima operazione $c_i=1$ se la tabella è non piena, mentre $c_i=i$ se la tabella è piena (copio $i$ elementi). Quindi $c_i=O(i)$ in generale. Invece nel caso peggiore, considerando n operazioni di costo $c_i$ avrò che il costo complessivo è $O(n^2)$. Infine per compiere un'analisi esatta (considerando solo le espansioni della tabella che realmente avvengono) avrò che il costo per $n$ operazioni è $<3n$.
		\end{myParagraph}

%PARAGRAFO 21
		\begin{myParagraph}{ALBERI BINARI DI RICERCA}
È una struttura dati spesso usata come dizionario e che supporta operazioni dinamiche. Esegue le operazioni base in $O(h)$ (dove $h$ è l'altezza dell'albero). Viene rappresentato con una struttura dati collegata $T$ dove ogni nodo è un oggetto.
			\begin{descr}{0}
				\item[Campi] $\rightarrow$
					$root$ (punta alla radice dell'albero),
					$key$ (il valore del nodo),
					$left$ (punta al figlio sinistro),
					$right$ (punta al figlio destro),
					$p$ (punta al padre($T.root$ non ha padre)).
				\item[Proprietà] $\rightarrow$
					se $y$ è nel sottoalbero sinistro di $x$ allora $y.key\leq x.key$,
					se $y$ è nel sottoalbero destro di $x$ allora $y.key\geq x.key$,
			\end{descr}
			\begin{code}{TreeInsert(T,z)}
\li $y\la\const{nil}$ \COMMENT{Tiene traccia del padre}
\li $x\la\attrib{T}{root}$
\li \WHILE{x\neq\const{nil}}
	\li $y\la x$
	\li \IF{\attrib{z}{key} < \attrib{x}{key}}
		\li $x\la\attrib{x}{left}$
	\li \ELSE $x\la\attrib{x}{right}$
	\End
\End
\li $\attrib{z}{p}\la y$
\li \IF{y=\const{nil}}
	\li $\attrib{T}{root}\la z$ \COMMENT{L'albero T era vuoto}
\li \ELSEIF{\attrib{z}{key} < \attrib{y}{key}}
	\li $\attrib{y}{left}\la z$
\li \ELSE $\attrib{y}{right}\la z$
			\end{code}
\bo{Costo}\\
Il costo dipende esclusivamente dall'altezza $h$ dell'albero, quindi il costo è $O(h)$. Nel caso migliore $h=\log_2 n$ mentre nel caso peggiore $h=n$. L'altezza dipende soltanto dall'ordine di inserimento delle chiavi.

			\begin{code}{TreeSearch(\attrib{T}{root},k) \qquad Versione Ricorsiva}
\li \IF{x=\const{nil} \const{ OR } k=\attrib{x}{key}}
	\li \RETURN $x$
\END
\li \IF{k < \attrib{x}{key}}
	\li \RETURN \proc{TreeSearch(\attrib{x}{left},$k$)}
	\li \ELSE \RETURN \proc{TreeSearch(\attrib{x}{right},$k$)}
\END
			\end{code}
\bo{Costo}\\
Anche in questo caso il costo dell'algoritmo dipende esclusivamente dall'altezza dell'albero, infatti il costo è $O(h)$.
			
			\begin{code}{TreeSearch(x,k) \qquad Versione Iterativa}
\li \WHILE{x\neq\const{nil} \const{ AND } k\neq\attrib{x}{key}}
	\li \IF{k < \attrib{x}{key}}
		\li $x\la\attrib{x}{left}$
	\li \ELSE $x\la\attrib{x}{right}$
	\END
\END
\li \RETURN $x$
			\end{code}
			
			\begin{code}{TreeMinimum(x)}
\li \WHILE{\attrib{x}{left}\neq\const{nil}}
	\li $x\la\attrib{x}{left}$
\END
\li \RETURN $x$
			\end{code}
La chiave minima si trova sempre nel nodo più a sinistra.
\bo{Costo }$\ra O(h)$
			\begin{code}{TreeMaximum(x)}
\li \WHILE{\attrib{x}{right}\neq\const{nil}}
	\li $x\la\attrib{x}{right}$
\END
\li \RETURN $x$
			\end{code}
La chiave massima si trova sempre nel nodo più a destra.
\bo{Costo }$\ra O(h)$
\break

\bo{Attraversamento di un Albero Binario} $\ra$ Per gli alberi binari è possibile effettuare un'esplorazione per stampare direttamente i valori ordinati (grazie alle sue proprietà). Esistono 3 modalità di "attraversamento" possibili, cioè:
			\begin{code}{InorderTreeWalk(x)}
\li \IF{x\neq\const{nil}}
	\li \proc{InorderTreeWalk(\attrib{x}{left})}
	\li	 \proc{Stampa} \attrib{x}{key}
	\li \proc{InorderTreeWalk(\attrib{x}{left})}
\END
			\end{code}
Per le altre due versioni, Preorder e Postorder, lo pseudocodice è lo stesso con l'unica differenza che la stampa del valore avviene rispettivamente prima o dopo le due chiamate ricorsive a "TreeWalk". \\
\bo{Costo}\\
Intuitivamente il costo è $\Theta(n)$ in quanto ogni nodo viene visitato una sola volta, ma è possibile dimostrare che il costo è $O(n)$.
\break

\bo{Successore/Predecessore di una Chiave} $\ra$ Il successore di $x$ è la più piccola chiave $>x.key$ mentre il predecessore di $x$ è la più grande chiave $<x.key$. E' anche possibile che $x.key$ sia il massimo/minimo dell'albero e che quindi non abbia successore/predecessore. Possiamo modellizzare il problema come:
			\begin{descr}{1}
				\item[$\bm{x}$ ha il sottoalbero destro non-vuoto] $\ra$ Il successore di $x$ è il minimo del sottoalbero destro di $x$;
				\item[$\bm{x}$ ha il sottoalbero destro vuoto] $\ra$ Il successore di $x$ è l'antenato più prossimo di $x$ che ha come figlio sinistro è anch'esso antenato di $x$.
			\end{descr}
Lo pseudocodice è:
			\begin{code}{TreeSuccessor(x)}
\li \IF{\attrib{x}{right}\neq\const{nil}}
	\li \RETURN \proc{TreeMinimum(\attrib{x}{right})}
\END
\li $y\la\attrib{x}{p}$
\li \WHILE{y\neq\const{nil} \const{ AND } x=\attrib{y}{right}}
	\li $x\la y$
	\li $y=\attrib{y}{p}$
\END
\li \RETURN $y$
			\end{code}
Per "TreePredecessor" l'algoritmo è lo stesso, basta cambiare "x.right" con "x.left".\\
\bo{Costo}\\
Il costo dipende anche in questo caso esclusivamente dell'altezza infatti è $O(n)$.
\break

\bo{Trapianto di Sottoalberi} $\ra$ Un trapianto sostituisce il sottoalbero con radice $u$ con quello di radice $v$ (\underline{N.B.} Non fa uno scambio). Lo pseudocodice è:
			\begin{code}{Transplant(T,u,v)}
\li \IF{\attrib{u}{p}=\const{nil}}
	\li $\attrib{T}{root}\la v$ \COMMENT{$u$ era la radice di T}
\li \ELSEIF{u=\attribb{u}{p}{left}}
	\li $\attribb{u}{p}{left}\la v$
\li \ELSE $\attribb{u}{p}{right}\la v$
\END
\li \IF{v\neq\const{nil}}
	\li $\attrib{v}{p}\la \attrib{u}{p}$
\END
			\end{code}
\bo{Cancellazione} $\ra$ Per effettuare la cancellazione di una chiave $z$ da un albero, possiamo trovarci un due casi, cioè:
			\begin{descr}{1}
				\item[$\bm{z}$ non ha un figlio] $\ra$ cancello $z$ e aggiusto il puntatore del padre di $z$.
				\item[$\bm{z}$ ha un figlio] $\ra$ cancello $z$ e faccio in modo che il padre di $z$ punti al figlio di $z$.
				\item[$\bm{z}$ ha due figli] $\ra$ devo trovare il successore di $z$ che chiamo $y$ che sarà il minimo del sottoalbero destro di $z$ e riordinare il sottoalbero, aggiustando i collegamenti con il padre di $z$.
			\end{descr}
			\begin{code}{TreeDelete(T,z)}
\li \IF{\attrib{z}{left}=\const{nil}}
	\li $\proc{Transplant}(T,z,\attrib{z}{right})$
\li \ELSEIF{\attrib{z}{right}=\const{nil}}
	\li $\proc{Transplant}(T,z,\attrib{z}{left})$
\li \ELSE 
	\li $y\la\proc{TreeMinimum}(\attrib{z}{right})$
	\li \IF{\attrib{y}{p}\neq z}
		\li $\proc{Transplant}(T,y,\attrib{y}{right})$
		\li $\attrib{y}{right}\la\attrib{z}{right}$
		\li $\attribb{y}{right}{p}\la y$
	\END
	\li $\proc{Transplant}(T,z,y)$
	\li $\attrib{y}{left}\la\attrib{z}{left}$ 
	\li $\attribb{y}{left}{p}\la y$
\END
			\end{code}
\bo{Chiavi duplicate} $\ra$ Se si hanno delle chiavi duplicate da inserire in un ABR nell'algoritmo "TreeInsert" è necessario aggiungere dei controlli, cioè: \\
\qquad $\bullet$ aggiungere $if(z.key=x.key) ...$ prima della riga 5; \\
\qquad $\bullet$ aggiungere $if(z.key=y.key) ...$ prima della riga 11. \\
Ci sono dunque diversi modi di assegnare chiavi duplicate come: fare un lista concatenata con valori uguali, aggiungere un flag booleano nel nodo $x$ e assegnare la chiave duplicata a $x.left$ o $x.right$ a seconda del valore del flag (che cambia ad ogni vista al nodo $x$) oppure assegnare casualmente la chiave duplicata ad $x.left$ o $x.right$.\\
\bo{Notazione Parentesizzata} $\ra$ È possibile rappresentare un ABR tramite una notazione alternativa. Ad esempio, avendo un albero composto da $x$, $x.left$ e $x.right$ posso scriverlo come:\\
			\begin{tcenter}
$x(x.left,x.right)$
			\end{tcenter}
Ponendo $x.left=y$ e $x.right=z$ e supponendo che entrambi abbiano solo un figlio sinistro posso scrivere:
			\begin{tcenter}
$x(y(y.left,\;),z(z.left,\;))$
			\end{tcenter}
		\end{myParagraph}

	\begin{myParagraph}{ALBERI AVL}
Gli alberi AVL sono alberi quasi bilanciati", infatti ,per qualsiasi nodo, le altezze dei due sottoalberi differiscono al più di 1. Questi alberi aggiungono anche un fattore di bilanciamento ad ogni nodo. Le sue proprietà principali sono:
		\begin{Descr}
			\item[Proprietà ereditate dagli ABR] $\ra$ gli alberi AVL ereditano tutte le proprietà degli ABR;
			\item[Altezza dei Nodi] $\ra$ ad ogni nodo si aggiunge un parametro "altezza" che indica appunto il \underline{massimo} numero di archi dal nodo considerato ad una foglia;
			\item[Fattore di Bilanciamento] $\ra$ ad ogni nodo $x$ si aggiunge un fattore di bilanciamento $\modulo{BF(x)}\leq 1$ e si calcola facendo $BF(x)=x.left.h-x.right.h$, è cioè la differenza tra l'altezza sottoalbero sinistro e l'altezza del sottoalbero destro.
			\item[Sentinella T.NIL] $\ra$ le foglie, ovvero i nodi ad altezza 0, non hanno una chiave e vengono rappresentate come un unico nodo, collegato a tutti i nodi ad altezza 1.
			\item[Altezza Massima] $\ra$ l'altezza massima di un AVL con $n$ nodi interni è $\bm{2\log_2 (n+1)}$;
			\begin{Descr}
				\item[Teorema]: Chiamo $n(h)$ il numero minimo di nodi che un albero di altezza $h$ deve avere. Per definizione avrò che: $n(h)\geq 2^{h/2}-1$ per ogni $h>1$.
				\item[Dimostrazione]:
				\begin{Descr}
					\item[h=1] $\ra$ $n(1)=1 > 2^{1/2}-1=\sqrt{2}-1$
					\item[h=2] $\ra$ $n(2)=2 > 2^{1/1}-1=1$
					\item[h$\bm{\geq}$2] $\ra$ (per induzione) suppongo che i due sottoalberi differiscano di un nodo, quindi uno avrà altezza $h-1$ e l'altro $h-2$. Applicando i la formula dei nodi minimi data dal teorema visto sopra avrò che:
					\begin{tcenter}
					$n(h)\geq \underbrace{1}_\text{nodo esaminato} + \underbrace{n(h-1)+n(h-2)}_\text{numero nodi sottoalberi}$ =\\
					=$\underbrace{1}_\text{nodo esaminato} + \underbrace{2^{h-1/2}-1+2^{h-2/2}-1)}_\text{numero nodi sottoalberi}$=\\
					=$(2^{-1/2}+2^{-1})2^{h/2}-1$=\\=$\frac{1}{2\sqrt{2}}2^{h/2}-1>2^{h/2}-1$.
					\end{tcenter}
					Ora ricavando $h$ rispetto ad $n$ ottengo: 
					\begin{tcenter}
						$n+1\geq2^{h/2}\implies\log_2(n+1)=h/2\implies h=2\log_2(n+1)$ \qquad C.V.D.
					\end{tcenter}
				\end{Descr}
			\end{Descr}
		\end{Descr} 
Gli AVL ereditano anche tutte le operazioni tipiche degli ABR, cioè MIN, MAX, SUCCESSOR, PREDECESSOR e SEARCH, ma il costo diventa dipendente dal numero di nodi infatti si eseguono in $O(log_2 \;n)$. Ci sono invece altre operazioni, come inserimento e cancellazione, che rischiano di violare le proprietà dell'AVL. A questo scopo si introducono le ROTAZIONI che permettono di ripristinare, se usate in modo corretto, il bilanciamento dei nodi. Lo pseudocodice è:
		\begin{code}{LeftRotate(T,x)}
\li $y \get \attrib{x}{right}$ \COMMENT{assegna y, che verrà ruotato insieme ad x}
\li $\attrib{x}{right}\get\attrib{y}{left}$
\li $\attrib{x}{h}\get \proc{max(\attribb{x}{left}{h},\attribb{x}{right}{h})}+1$
\li \IF{y.left \neq \attrib{T}{\const{nil}}}
	\li $\attribb{y}{left}{p}\get x$ \COMMENT{collega x e il sottoalbero sinistro di y}
\END
\li $\attrib{y}{p}\get\attrib{x}{p}$ \COMMENT{collega il padre di x come padre di y}
\li \IF{\attrib{x}{p}=\attrib{T}{\const{nil}}} \qquad\qquad \COMMENT{collega il padre di x a y come padre}
	\li $\attrib{T}{root}\get y$
\li \ELSEIF{x\gets\attribb{x}{p}{left}} 
	\li $\attribb{x}{p}{left}\get y$
\li \ELSE
	\li $\attribb{x}{p}{right}\get y$
\END
\li $\attrib{y}{left}\get x$ \COMMENT{collega x come figlio di y}
\li $\attrib{y}{h}\get\proc{max(\attribb{y}{left}{h},\attribb{y}{right}{h})}+1$
\li $\attrib{x}{p}\get y$ \COMMENT{collega y come padre di x}
		\end{code}
		\begin{code}{\const{avl}Insert(T,z)}
\li $y\get\attrib{T}{\const{nil}}$ \COMMENT{tiene traccia del padre}
\li $x\get\attrib{T}{root}$
\li \WHILE{x\neq\attrib{T}{\const{nil}}} \qquad\qquad\quad\COMMENT{scorre fino a trovare la foglia dove inserire x}
	\li $y\get x$
	\li \IF{\attrib{z}{key}<\attrib{x}{key}}
		\li $x\get\attrib{x}{left}$
	\li \ELSE
		\li $x\get \attrib{x}{right}$
	\END
\END
\li $\attrib{z}{p}\get y$ \COMMENT{assegna il padre alla chiave da inserire}
\li \IF{y\gets\attrib{T}{\const{nil}}} \qquad\qquad\quad\COMMENT{Assegna z come il giusto figlio di y}
	\li $\attrib{T}{root}\get z$
\li \ELSEIF{\attrib{z}{key}<\attrib{y}{key}}
	\li $\attrib{y}{left}\get z$
\li \ELSE
	\li $\attrib{y}{right}\get z$
\END
\li $\attrib{z}{left}\get\attrib{T}{\const{nil}}$\COMMENT{Crea le due foglie \const{nil} di z}
\li $\attrib{z}{right}\get\attrib{T}{\const{nil}}$
\li $\attrib{z}{h}\get 1$
\li $\proc{AVLInsertFixup}(T,z)$\COMMENT{chiamata alg. che aggiusta il bilanciamento}
		\end{code}
Quando si inserisce/cancella una nuova foglia $z$ bisogna aggiustare il bilanciamento di qualche $x$ antenato di $z$. Infatti esisterà almeno un nodo $x$ per cui $-2\leq BF(x)\leq2$, violando cioè le regole degli AVL. Esistono quindi due possibilità:
Posso dividere il problema in 4 sottocasi:
		\begin{descr}{1}
			\item[se $\bm{BF(x)=2}$] $\ra$ il sottoalb. sinistro è due livelli più alto di quello destro;
			\begin{Descr} 
				\item[$\bm{z}$ inserito nel sottoalb. x.left.left] $\ra$ allora avrò che $BF(x.right)=1$ $\implies$ \underline{RightRotate su $x$};
				\item[$\bm{z}$ inserito nel sottoalb. x.left.right] $\ra$ allora avrò che $BF(x.left)=-1$ $\implies$ \underline{LeftRotate su $y$} poi \underline{RightRotate su $x$}.
			\end{Descr}
			\item[se $\bm{BF(x)=2}$] $\ra$ il sottoalb. destro è due livelli più alto di quello sinistro;
			\begin{Descr} 
				\item[$\bm{z}$ inserito nel sottoalb. x.right.left] $\ra$ allora avrò che $BF(x.right)=1$ $\implies$\underline{LeftRotate su $x$};
				\item[$\bm{z}$ inserito nel sottoalb. x.right.right] $\ra$ allora avrò che $BF(x.left)=-1$ $\implies$\underline{RightRotate su $y$} poi \underline{LeftRotate su $x$};
			\end{Descr}
		\end{descr}
Lo pseudocodice è:
		\begin{code}{\const{AVL}InsertFixup(T,x)}
\li $x\get\attrib{x}{p}$
\li \WHILE{x\neq\attrib{T}{\const{nil}}}
	\li $\attrib{x}{h}\get\proc{max(\attribb{x}{left}{h},\attribb{x}{right}{h})}+1$
	\li \IF{(\attribb{x}{left}{h}-\attribb{x}{right}{h})=2}
		\li \IF{(\attribbb{x}{left}{left}{h}-\attribbb{x}{left}{right}{h})=-1}
			\li $\proc{LeftRotate}(T,x.left)$
		\END
		\li $\proc{RightRotate}(T,x)$
		\li $x\get\attrib{x}{p}$
	\li \ELSEIF{(\attribb{x}{left}{h}-\attribb{x}{right}{h})=-2}
		\li \IF{(\attribbb{x}{right}{left}{h}-\attribbb{x}{right}{right}{h})=1}
			\li $\proc{RightRotate}(T,x.right)$
		\END
		\li $\proc{LeftRotate}(T,x)$
		\li $x\get\attrib{x}{p}$
	\END
	\li $x\get\attrib{x}{p}$
\END
		\end{code}
\bo{Costo} \\
Il costo di \proc{AVLInsertFixup} è $O(1)$ per ogni iterazione, essendo $O(\log_2 n)$ iterazioni (corrispondenti al numero di livelli dell'albero) il costo totale di \proc{AVLInsertFixup} è $O(\log_2 n)$. Il costo di \proc{AVLInsert} è $O(\log_2 n)$ fino all'esecuzione di \proc{AVLInsertFixup}. Il costo complessivo è comunque $O(\log_2 n)$.\\
\bo{Correttezza con I.C.}\\
"All'inizio di ogni iterazione, in AVL c'è al massimo una violazione del bilanciamento di AVL" (non è necessario dimostrarla).
	\end{myParagraph}
	\begin{myParagraph}{STRUTTURE DATI AUMENTATE}
Raramente progetto una struttura dati da zero, ma spesso ho bisogno di aggiungere informazioni e quindi anche nuove operazioni ad una struttura dati esistente. L'aspetto fondamentale delle strutture dati aumentate è quello di aggiungere nuove operazioni senza perdere di efficienza.
		\begin{subParagraph}{Statistiche d'Ordine Dinamiche}
Voglio aggiungere le funzioni:
			\begin{descr}{1}
				\item[\proc{\bo{OS-Select}}(x,i)] $\ra$ ritorna il puntatore al nodo che contiene la $i$-esima chiave più piccola del sottoalb. con radice $x$;
				\item[\proc{\bo{OS-Rank}}(T,x)] $\ra$ ritorna la posizione (rango) di $x$ nell'ordine determinato da un attraversamento Inorder di $T$.
			\end{descr}
Posso usare l'attraversamento Inorder, ma questo comporta un aumento del costo (che diventa $O(n)$) oppure posso aumentare la struttura aggiungendo $\bm{x.size}$ cioè il numero di nodi nel sottoalb. con radice $x$ (includo $x$ ma escludo le foglie \const{nil}). Avrò dunque che:
			\begin{tcenter}
			$\attrib{x}{size}\get\attribb{x}{left}{size}+\attribb{x}{right}{size}+1$
			\end{tcenter}
Gli pseudocodici sono:
			\begin{code}{OS-Select(x,i)}
\li $r\get \attribb{x}{left}{size} +1$ 
\li \IF{i\gets r}
	\li \RETURN $x$
\li \ELSEIF{i<r}
	\li \RETURN \proc{OS-Select($\attrib{x}{left},i$)}
\li \ELSE
\li \RETURN \proc{OS-Select($\attrib{x}{right},i-r$)}
\END
			\end{code}	
\bo{Costo}\\
Ad ogni chiamata ricorsiva scende di un livello nell'albero, che ha $O(\log_2 n)$ livelli, quindi il costo complessivo è $O(\log_2 n)$.\\
\bo{Correttezza}\\
Poiché è ricorsivo in coda posso considerarlo come un alg. iterativo e usare l'I.C. per dimostrarne la correttezza:\\
"Prima di ogni iterazione l'$i$-esimo valore si trova nel sottoalb. con radice in $x$".\\
Dato $r$ rango di $x$ avrò che:
			\begin{descr}{1}
				\item[$\scalare$] se $i=r \ra$ ritorna $x\ra$ OK;
				\item[$\scalare$] se $i<r \ra i$-esimo elemento più piccolo è nel sottoalb. sinistro;  
				\item[$\scalare$] se $i>r \ra i$-esimo elemento più piccolo è nel sottoalb. destro $\ra$ sottraggo gli $r$ elementi che precedono quelli nel sottoalb. destro di $x$.
			\end{descr}
			\begin{code}{OS-Rank(T,x)}
\li $r\get\attribb{x}{left}{size}+1$
\li $y\get x$
\li \WHILE{y\neq\attrib{T}{root}}
	\li \IF{y=\attribb{y}{p}{right}}
		\li $r\get\attribbb{y}{p}{left}{size}+1$
	\END
	\li $y\get\attrib{y}{p}$
\END
\li \RETURN $r$
			\end{code}
\bo{Costo}\\
$y$ sale di un livello ad ogni iterazione quindi anche in questo caso il costo è $O(\log_2 n)$.\\
\bo{Correttezza con I.C.}\\
"All'inizio di ogni iterazione del ciclo ho che $r=\proc{Rank}(x.key,y)$ "
		\end{subParagraph}
		\begin{subParagraph}{Attributo Size}
Suppongo di aggiungere alla struttura degli AVL un attributo "$size$". L'obbiettivo è quello di mantenere il costo della struttura, cioè $O(\log_2(n))$. Per farlo, senza aggiungere una funzione per aggiustare la "$size$" dei sottoalberi posso direttamente farlo dopo Inserimento e Cancellazione. Inoltre anche le rotazioni modificano "$size$".
			\begin{descr}{1}
				\item[Inserimento]:\\
				\begin{itemize} 
					\item incrementa "$size$" di ogni nodo visitato;
					\item nel fixup modifica "$size$" dopo le rotazioni.
				\end{itemize}
Dopo $\proc{leftRotate}$\\
...\\
$\attrib{y}{size} \get \attrib{x}{size}$\\
$\attrib{x}{size} \get \attribb{x}{left}{size}+\attribb{x}{right}{size}+1$
	
Dopo $\proc{rightRotate}$\\
...\\
$\attrib{x}{size} \get \attrib{y}{size}$\\
$\attrib{y}{size} \get \attribb{y}{left}{size}+\attribb{y}{right}{size}+1$			
				\item[Cancellazione] :\\
				\begin{itemize} 
					\item decrementa "$size$" dal nodo cancellato a $T.root$;
					\item nel fixup modifica "$size$" dopo le rotazioni (senza entrare nel dettaglio).
				\end{itemize}
			\end{descr}
		\end{subParagraph}
		\begin{subParagraph}{Come Aumentare una Struttura Dati}
\bo{Teorema}\\
Se si aumenta un albero AVL con un campo $f$, dove $x.f$ dipende solo da informazioni in $x$, $x.left$, $x.right$ allora si possono mantenere i valori di $f$ in tutti i nodi in $O(\log_2(n))$.\\
\bo{Dimostrazione}\\
La modifica di $x.f$ si propaga solo agli antenati di $x$, che vengono aggiornati al costo di $O(1)$ ciascuno, in totale $O(\log_2(n))$.\\
\bo{Applicazione}\\
			\begin{descr}{1}
				\item[Inserimento]:\\
				\begin{itemize} 
					\item aggiusto la statistica $f$ in tutti i nodi visitati (non sempre posso farlo in scendendo ma posso sempre farlo salendo);
					\item aggiusto la statistica $f$ dopo tutte le rotazioni sui nodi x,y,padre (costo O(1)) e se necessario la aggiusto anche su tutti i predecessori (costo complessivo $O(log_2(n))$).
				\end{itemize}
				\item[Cancellazione]:\\
				\begin{itemize} 
					\item aggiusto la statistica $f$ in tutti gli antenati del nodo rimosso;
					\item aggiusto la statistica dopo tutte le rotazioni (al massimo 3) sui nodi x,y,padre (costo O(1)) e se necessario la aggiusto anche su tutti i predecessori (costo complessivo $O(log_2(n))$).
				\end{itemize}
			\end{descr}
		\end{subParagraph}
		\begin{subParagraph}{Alberi di Intervalli}
Servono a gestire un insieme di intervalli. Si aumenta quindi una struttura dati aggiungendo l'attributo $x.int$ che a sua volta ha attributi $int.low$ e $int.high$.\\
Uso per comodità gli alberi AVL e impongo $x.key$ come l'estremo inferiore dell'intervallo (affinché un InorderTreeWalk stampi gli intervalli in ordine rispetto all estremo inferiore), dunque $x.int$ conterrà solo l'estremo superiore dell'intervallo. Inoltre aggiungo anche un attributo $x.max=\max(x.int,\; x.left.max,\; x.right.max)$ che rappresenta il massimo estremo superiore presente nel sottoalbero con radice $x$. Come fatto anche per l'attributo $size$, posso aggiornare $x.max$ durante la discesa per l'inserimento in tempo $O(1)$ per ogni rotazione che effettuo.
			\begin{Descr} 
				\item[Sovrapposizione di intervalli]: Si dice che due intervalli $i,j$ si sovrappongono sse $i.low\leq j.high$ AND $j.low\leq i.high$ mentre non si sovrappongono sse $i.low>j.high$ OR $j.low> i.high$.
				\item[Sviluppo di nuove operazioni]: Utilizzando gli attributi dei nodi appena visti è possibile sviluppare nuove operazioni come ad esempio la ricerca. Lo pseudocodice è:
				\begin{code}{IntervalSearch(T,i)}
\li $x \get \attrib{T}{root}$
\li \WHILE{x\neq\attrib{T}{\const{nil}} \text{ AND } i \text{ non si sovrappone a }\attrib{x}{int}}
	\li \IF{\attrib{x}{left} \neq \attrib{T}{\const{nil}} \text{ AND } \attribb{x}{left}{max} \geq \attrib{i}{low}}
		\li $x \get \attrib{x}{left}$
	\li \ELSE
		\li $x \get \attrib{x}{right}$
	\END
\END
\li \RETURN $x$
				\end{code}
\bo{Costo}\\
Ho costruito la struttura dati affinché il costo sia $O(\log_2 n)$
\bo{Correttezza}\\
L'algoritmo restituisce un nodo il cui intervallo si sovrapponga con $i$ oppure restituisce $\attrib{T}{\const{nil}}$ se non ci sono nodi il cui intervallo si sovrapponga con $i$. In altre parole se la ricerca va a destra (sinistra) allora c'è un intervallo che si sovrappone con $i$ nel sottoalb. destro (sinistro) oppure non esiste un intervallo che si sovrapponga con $i$.\\
(Vedi dimostrazione ...)
			\end{Descr}
		\end{subParagraph}
	\end{myParagraph}
	
	\begin{myParagraph}{PROGRAMMAZIONE DINAMICA}
Fino a questo momento ho usato la logica "Divide et Impera" poiché scomponendo un problema ottenevo sottoproblemi indipendenti. La programmazione dinamica invece si occupa di problemi che hanno sottoproblemi non indipendenti, comuni a più parti del problema. Questi sottoproblemi vengono salvati in una tabella per essere riutilizzati più volte. Spesso viene usata nell'ambito dell' ottimizzazione. La procedura per la risoluzione di questo tipo di sottoproblemi è:
			\begin{itemize} 
				\item individuare la struttura della soluzione ottima;
				\item definire ricorsivamente la soluzione ottima;
				\item calcolare la soluzione ottima in modo bottom-up (dai sottoproblemi più semplici a quelli più complessi);
				\item costruire una soluzione ottima del problema richiesto.
			\end{itemize}
		\begin{subParagraph}{Taglio delle Aste}
Supponiamo che il prezzo di un'asta dipenda dalla sua lunghezza, un'asta lunga $n$ può essere tagliata in pezzi più corti che hanno valore diverso. L'obbiettivo è trovare i tagli che permettano di massimizzare il ricavato. \\
Per risolvere questo problema utilizzando la programmazione dinamica devo:
			\begin{itemize}
				\item Individuare la struttura della soluzione ottima:\\
				\begin{Descr}
					\item[\bm{$r_n=r_i+r_{n-i}$}] $\get$ suddivido il problema in due sottoproblemi dove $r_n$ è il ricavo ottimo che voglio ottenere, e $r_i,r_{n-i}$ sono i due ricavi parziali ottimi ottenuti da un taglio in posizione $i$. La soluzione si ottiene quindi calcolando \bm{$\max\;(p_n, r_1+r_{n-1}, ... , r_{n-1}+r_1)$};
					\item[\bm{$r_n=p_i+r_{n-i}$}] $\get$ scompongo il problema in una costante $p_i$, ovvero il prezzo di un primo pezzo di asta tagliata in $i$ e il ricavato ottimo della restante parte dell'asse. La soluzione si ottiene calcolando \bm{$\max\limits_{1\leq i\leq n}(p_i+r_{n-i})$}.
				\end{Descr}
				\item Definire ricorsivamente la soluzione ottima:\\
				\begin{Descr}
					\item[Usando la 2° formulazione] Posso definire un algoritmo ricorsivo che risolva il problema. Lo pseudocodice è: 
					\begin{code}{CutRod(p,n)}
\li \IF{n=0}
	\li \RETURN $0$
\END
\li $q \get -\infty$
\li \FOR{i \get 1 \TO n}
	\li $q \get \proc{max}(q, p[i]+\proc{CutRod}(p, n-i))$
\END
\li \RETURN $q$
					\end{code}
				\end{Descr}
				\bo{Costo}\\
				Considerando tutti i possibili tagli dell'asta, ovvero $2^{n-1}$, il costo del problema è esattamente $T(n)=\Theta(2^n)$ poiché si risolvono tutti i sottoproblemi, anche quelli che si presentano più volte. 
				\item Calcolare la soluzione ottima (2 possibili metodi):\\
Memorizzo i problemi risolti in una tabella affinché possano essere riutilizzati nel caso si presentassero più di una volta.
				\begin{Descr}
					\item[Top-Down]: Risolvo il problema di dimensione $n$ e ricorsivamente risolvo i sottoproblemi più piccoli. Prima di effettuare la ricorsione verifico se nella tabella è già presente la soluzione del problema che voglio risolvere. Lo pseudocodice è:
				\end{Descr}
				\begin{code}{MemoizedCutRod(p,n)}
\li Sia $r[0,...,n]$ un nuovo array
\li \FOR{i \get 0 \TO n}
	\li $r[i] \get -\infty$
\END
\li \RETURN \proc{MemoizedCutRodAux} (p,n,r)
				\end{code}
				\begin{code}{MemoizedCutRodAux(p,n,r)}
\li \IF{r[n] \geq 0}
	\li \RETURN r[n]
\END
\li \IF{n=0}
	\li $q=0$
\li \ELSE
	\li $q \get -\infty$
	\li \FOR{i \get 1 \TO n}
		\li $q \get \proc{max}(q, p[i]+\proc{MemoizedCutRodAux}(p, n-i, r))$
	\END
\END
\li $r[n] \get q$
\li \RETURN $q$
				\end{code}
\bo{Costo}\\
Ogni sottoproblema è risolto una sola volta, avrò in totale $n+(n-1)+(n-2)+\cdots+1=\frac{n(n+1)}{2}$ iterazioni. Il costo asintotico è quindi $\Theta(n^2)$.
				\begin{Descr}
					\item[Bottom-Up]: Risolvo i sottoproblemi partendo dai più piccoli, in modo che arrivando alla dimensione $n$ tutti i problemi di dimensione $<n$ sono già stati risolti e memorizzati. Lo pseudocodice è: 
				\end{Descr}
				\begin{code}{BottomUpCutRod(p,n)}
\li Sia $r[0,...,n]$ un nuovo array
\li $r[0] \get 0$
\li \FOR{j \get 1 \TO n}
	\li $q \get -infty$
	\li \FOR{i \get 1 \TO j}
		\li $q \get \proc{max}(q, p[i]+r[j-i])$
	\END
	\li $r[j] \get q$
\END
\li \RETURN $r[n]$
				\end{code}
				\bo{Costo}\\
Svolgo due cicli annidati, avrò in totale $n+(n-1)+(n-2)+\cdots+1=\frac{n(n+1)}{2}$ iterazioni. Il costo asintotico è quindi $\Theta(n^2)$.
			\end{itemize}
Gli algoritmi appena visti tengono traccia solo del massimo ricavo ma non dei tagli da effettuare per ottenerlo. Aggiungo un'array $s$ che memorizzi i tagli ottimi. Lo pseudocodice è:
			\begin{code}{ExtendedBottomUpCutRod(p,n)}
\li Siano $r[0,...,n]$ ed $s[0,...,n]$ due nuovi array
\li $r[0] \get 0$
\li \FOR{j \get 1 \TO n}
	\li $q \get -\infty$
	\li \FOR{i \get 1 \TO j}
		\li \IF{q<p[i]+r[j-i]}
			\li $q \get p[i]+r[j-i]$
			\li $s[j] \get i$
		\END
	\END
	\li $r[j] \get q$
\END
\li \RETURN $(r,s)$
			\end{code}
			\begin{code}{PrintCutRodSolution(p,n)}
\li $(r,s) \get \proc{ExtendedBottomUpCutRod(p,n)}$
\li \WHILE{n>0}
	\li print $s[n]$
	\li $n \get n-s[n]$
\END
			\end{code}
		\end{subParagraph}
	
		\begin{subParagraph}{LCS - Longest Common Sequence}
Date 2 sequenze $X=<x_1,...,x_m>$ e $Y=<y_1,...,y_n>$ trovare una sottosequenza comune ad $X,Y$ di lunghezza massima. Una sottosequenza è una sequenza a cui tolgo $0$ o più elementi, infatti $Z=<1,...,z_k>$ è una sottosequenza di $X$ se $\exists$ una sequenza di indici $I=<1,...,k>$ tali che $i_j>i_h$ se $j>h\;\forall j,h=1,...,k$ e $x_{i_j}=z_j \; \forall j=1,...,k$.\\
Per trovare la LCS tra $X$ e $Y$ potrei usare l'algoritmo "forza-bruta" analizzando tutte le possibili sottosequenze ma il costo sarebbe $\Theta(n\cdot 2^m)$ (molto costoso). Posso in alternativa ricorrere alla programmazione dinamica:
			\begin{itemize}
		\item Individuare la struttura della soluzione ottima:\\
				\begin{Descr}
			\item[Notazione] $\ra$ Definisco il prefisso di una sequenza $X$, detto $X_i$, ovvero i primi $i$ elementi della sequenza $X$.
			\item[Teorema:] Date $X,Y$ due sequenze e $Z$ una loro qualunque LCS, avrò che:
					\begin{Descr}
				\item[1)] se $x_m=y_n \implies z_k=x_m=y_n$ e $Z_{k-1}$ è una LCS di $X_{m-1}$ e $Y_{n-1}$;
				\item[2)] se $x_m \neq y_n \implies$ se $z_k \neq x_m$, $Z$ è una LCS di $X_{m-1}$ e $Y$;
				\item[3)] se $x_m \neq y_n \implies$ se $z_k \neq y_n$, $Z$ è una LCS di $X$ e $Y_{n-1}$.
					\end{Descr}
				\end{Descr}
		\item Definire ricorsivamente la soluzione ottima:\\
Posso distinguere due casi:
				\begin{Descr}
			\item[\bm{$x_m = y_n$}] $\ra$ devo risolvere un solo sottoproblema, ovvero trovare una LCS di $X_{m-1}$ e $Y_{n-1}$;
			\item[\bm{$x_m \neq y_n$}] $\ra$ devo risolvere due sottoproblemi, ovvero trovare una LCS di $X_{m-1}$ e $Y$ e una LCS di $X$ e $Y_{n-1}$-. La più lunga delle due sarà la LCS di $X$ e $Y$.
 				\end{Descr}
Utilizzo una matrice $c$ per memorizzare la LCS, $c[i,j]$ sarà la lunghezza della LCS di $X_i$ e $Y_j$. L'obbiettivo è trovare $c[m,n]$ ovvero la lunghezza della LCS di $X$ e $Y$. \\
\small{$c[i,j]=		
\begin{cases}
		0 \qquad \text{se } i=0 \text{ o } j=0 \\
		c[i-1,j-1]+1 \qquad \text{se } i,j>0 \text{ e } x_i=y_j \\
		\max (c[i,j-1]\;,\;c[i-1,j]) \qquad \text{se } i,j>0 \text{ e } x_i \neq y_j \\
\end{cases}$}
Il vantaggio di questa formulazione è che al massimo devo risolvere due sottoproblemi mentre per il taglio delle aste li consideravo tutti. Inoltre, anche in questo caso, memorizzo i sottoproblemi comuni per evitare di calcolarli più di una volta.
		\item Calcolare la lunghezza della soluzione ottima:\\
Creo un algoritmo che compila le tabelle $b$, che indica la direzione che ho seguito per risolvere i sottoproblemi (se $b[i,j]=\nwarrow$ ho esteso la LCS di un carattere), e $c$, che indica passo passo la lunghezza della LCS. Lo psudocodice è:
				\begin{code}{LCS-Length(X,Y)}
\li $m \get X.length$
\li $n \get Y.length$
\li Siano $b[1,...,m\; ; \;1,...,n] \text{ e } c[0,...,m\; ; \;0,...,n]$ due nuove tabelle
\li \FOR{i \get 1 \TO  m}
	\li $c[i,0] \get 0$
\END
\li \FOR{j \get 0 \TO  n}
	\li $c[0,j] \get 0$
\END
\li \FOR{i \get 1 \TO m}
	\li \FOR{j \get 1 \TO n}
		\li \IF{x_i = y_j}
			\li $c[i,j] \get c[i-1,j-1]+1$
			\li $b[i,j] \get "\nwarrow"$
		\li \ELSEIF{c[i-1,j] \geq c[i,j-1]}
			\li $c[i,j] \get c[i-1,j]$
			\li $b[i,j] \get "\uparrow"$
		\li \ELSE
			\li $c[i,j] \get c[i,j-1]$
			\li $b[i,j] \get "\la"$
		\END
	\END
\END
\li \RETURN $b,c$
				\end{code}
		\item Costruire una soluzione ottima:\\
Usando le 2 tabelle $b$ e $c$ posso scrivere un algoritmo che individui e stampi la LCS. Lo pseudocodice è:
				\begin{code}{Print-LCS(b,X,i,j)}
\li \IF{i=0 \text{ OR } j=0}
	\li \RETURN
\END
\li \IF{b[i,j]="\nwarrow"}
	\li $\proc{Print-LCS}(b,X,i-1,j-1)$
	\li print $x_i$
\li \ELSEIF{b[i,j]="\uparrow"}
	\li $\proc{Print-LCS}(b,X,i-1,j)$
\li \ELSE
	\li $\proc{Print-LCS}(b,X,i,j-1)$
\END
				\end{code}
			\end{itemize}
		\end{subParagraph}

		\begin{subParagraph}{Edit Distance}
L'obbiettivo è quello di trasformare la stringa $X$ nella stringa $Y$ usando solo operazioni elementari, come Inserimento, Cancellazione e Sostituzione di un carattere. Definisco $X=<x_1,...,x_i>$ e $Y=<y_1,...,y_j>$. Voglio minimizzare il numero delle operazioni per trasformare una stringa nell'altra, il loro numero minimo è la distanza tra le 2 stringhe. L'idea generale è quella di usare una tabella $c$ per memorizzare i costi di tutte le successive operazioni, infatti,supponendo di conoscere il costo dell'ultima operazione, basterà aggiungere il costo della successiva. Esistono 6 casi distinti:
			\begin{descr}{1}
				\item[Copia] $\ra$ allora era $x_i=y_j$ e il sottoproblema rimanente è $X_{i-1}\to Y_{j-1}$ quindi il costo sarà $c[i,j]=c[i-1,j-1]+\text{ "costo copia"}$;
				\item[Sostituzione] $\ra$ allora era $x_i \neq y_j$ e il sottoproblema rimanente è $X_{i-1}\to Y_{j-1}$ quindi il costo sarà $c[i,j]=c[i-1,j-1]+\text{ "costo sostituzione"}$;
				\item[Scambio] $\ra$ allora era $x_i=y_{j-1}$ e $y_j=x_{i-1}$ e il sottoproblema rimanente è $X_{i-2}\to Y_{j-2}$ quindi il costo sarà $c[i,j]=c[i-2,j-2]+\text{ "costo scambio"}$;
				\item[Cancellazione] $\ra$ allora non ho restrizioni su $X,Y$ e il sottoproblema rimanente è $X_{i-1}\to Y_j$ quindi il costo sarà $c[i,j]=c[i-1,j]+\text{ "costo cancellazione"}$;
				\item[Inserimento] $\ra$ allora non ho restrizioni su $X,Y$ e il sottoproblema rimanente è $X_i\to Y_{j-1}$ quindi il costo sarà $c[i,j]=c[i,j-1]+\text{ "costo inserimento"}$;
				\item[Casi Base] $\ra$ se $i=0$ o $j=0$ allora $X_0$ o $Y_0$ è una stringa vuota, quindi posso trasformare $X\to Y$ con $j$ inserimenti o $i$ cancellazioni. I rispettivi costi saranno:\\
$c[0,j]=j\cdot$ "costo inserimento"\\
$c[i,0]=i\cdot$ "costo cancellazione"\\
			\end{descr}		
Quindi usando le formule precedenti:\\
			\begin{tcenter}
\small{$c[i,j]=\min		
\begin{cases}
c[i-1,j-1]+\text{cost}\proc{(copy)}\qquad\text{se }x[i]=y[j]\\
c[i-1,j-1]+\text{cost}\proc{(replace)}\qquad\text{se }x[i]\neq y[j]\\
c[i-2,j-2]+\text{cost}\proc{(twiddle)}\qquad\text{se }x,y\geq 2\text{ AND } x[i-1]=y[j]\\
c[i-1,j]+\text{cost}\proc{(delete)}\qquad\text{sempre}\\
c[i-1,j]+\text{cost}\proc{(insert)}\qquad\text{sempre}\\
\end{cases}$}
			\end{tcenter}
Scrivo quindi un algoritmo che usa le due tabelle $c$, dei costi, e $op$ delle operazioni. Lo pseudocodice è:
			\begin{code}{EditDistance(X,Y)}
\li $m \get X.length$
\li $n \get Y.length$
\li Siano $c[0,...,m\; ; \;0,...,n]$ e $op[0,...,m\; ; \;0,...,n]$ due nuove tabelle
\li \FOR{i \get 0 \TO m}
	\li $c[i,0] \get i\cdot \proc{cost}(\text{DELETE})$
	\li $op[i,0] \get$ "DELETE"
\END
\li \FOR{j \get 0 \TO n}
	\li $c[0,j] \get j\cdot \proc{cost}(\text{INSERT})$
	\li $op[0,j] \get$ "INSERT"
\END
\li \FOR{i \get 1 \TO m}
	\li \FOR{j \get 1 \TO n}
		\li $c[i,j] \get \infty$
		\li \IF{x_i=y_j}
			\li $c[i,j] \get c[i-1,j-1]+\proc{cost}(\text{COPY})$
			\li $op[i,j] \get$ "COPY"
		\END
		\li \IF{x_i \neq y_j \text{ AND } c[i-1,j-1]+\proc{cost}(\text{REPLACE})<c[i,j]}
			\li $c[i,j] \get c[i-1,j-1]+\proc{cost}(\text{REPLACE})$
			\li $op[i,j] \get$ "REPLACE"
		\END
		\li \IF{i,j \geq 2 \text{ AND } x_i=y_{j-1} \text{ AND }y_j=x_{i-1} \text{ AND }...}
			\li$...\text{ AND }c[i-2,j-2]+\proc{cost}(\text{TWIDDLE})<c[i,j]$
			\li $c[i,j] \get c[i-2,j-2]+\proc{cost}(\text{TWIDDLE})$
			\li $op[i,j] \get$ "TWIDDLE"
		\END
		\li \IF{c[i-1,j]+\proc{cost}(\text{DELETE})<c[i,j]}
			\li $c[i,j] \get c[i-1,j]+\proc{cost}(\text{DELETE})$
			\li $op[i,j] \get$ "DELETE"
		\END
		\li \IF{c[i,j-1]+\proc{cost}(\text{INSERT})<c[i,j]}
			\li $c[i,j] \get c[i,j-1]+\proc{cost}(\text{INSERT})$
			\li $op[i,j] \get$ "INSERT"
		\END
	\END
\END
\li \RETURN $c,op$
			\end{code}
\bo{Costo}\\
Il costo dell'$\proc{EditDistance}$ in termini di tempo è $\Theta(n\cdot m)$, dovuto ai due cicli annidati, lo stesso è il costo di spazio di memoria che infatti è $\Theta(n\cdot m)$.\\
Usando la tabella $op$ restituita da $\proc{EditDistance}$ posso creare un algoritmo che restituisca la serie di operazioni. Lo pseudocodice è:
			\begin{code}{OpSequence(op,i,j)}
\li \IF{i=0 \text{ AND }j=0}
	\li \RETURN
\END
\li \IF{op[i,j]=\text{"COPY" OR }op[i,j]=\text{"REPLACE"}}
	\li $i' \get i-1$
	\li $j' \get j-1$
\li \ELSEIF{op[i,j]=\text{"TWIDDLE"}}
	\li $i' \get i-2$
	\li $j' \get j-2$
\li \ELSEIF{op[i,j]=\text{"DELETE"}}
	\li $i' \get i-1$
	\li $j' \get j$
\li \ELSE \COMMENT{Ovvero op[i,j]=\text{"INSERT"}}
	\li $i' \get i$
	\li $j' \get j-1$
\END
\li $\proc{OpSequence}(op,i',j')$
\li $\proc{print}(op[i,j])$
			\end{code}
Gli utilizzi della $\proc{EditDistance}$ sono molti, ad esempio: correzione automatica di parole errate (individuate grazie al contesto), suggerimenti di ricerca o correzione ortografica. Al questo scopo è spesso necessario confrontare una stringa con un insieme molto grande di stringhe, come ad esempio un dizionario o tutte le parole sul web, ma questo implicherebbe un costo proibitivo. Esiste modo per ridurre l'insieme dei candidati che esamino e si chiama "Intersezione di n-gram". Consiste nell'associare alla parola di partenza numerose sequenze di caratteri che abbiano una $\proc{EditDistance}$ minore di una certa soglia. Dal dizionario si estraggono solo parole che contengano abbastanza di quegli "n-gram". Per avere una sovrapposizione di n-gram normalizzata (utile per confronti molto grandi) posso usare il coefficente di Jaccard:\\
			\begin{tcenter}
$JC=\frac{\modulo{X\bigcap Y}}{\modulo{X\bigcup Y}}$ con $X,Y$ insiemi di dimensione diversa. 
			\end{tcenter}
		\end{subParagraph}
	\end{myParagraph}
	
	\begin{myParagraph}{ALGORITMI ELEMENTARI PER GRAFI}
Dato un grafo $G=(V,E)$, dove $V$ è l'insieme dei vertici ed $E$ è l'insieme degli archi, posso definire il \bo{Grado} $u.degree$ di un vertice come il numero di archi che convergono in esso.
		\begin{Descr}
			\item[Grafo Diretto] $\ra$
E' un grafo i cui archi sono coppie di vertici ordinati;
			\item[Grafo Indiretto] $\ra$
E' un grafo i cui archi sono coppie di vertici \underline{non} ordinati;
			\item[Cammino] $\ra$ Un cammino di lunghezza $k$ è una sequenza di vertici $<v_0,v_1,...,v_k>$ tale che $(v_{i-1},v_i)\in E$ per $i=0,...,k$. 
			\begin{Descr}
				\item[Ciclo] $\ra$ in un grafo orientato, un cammino $<v_0,v_1,...,v_k>$ forma un ciclo se $v_0=v_k$ e il cammino contiene almeno un arco;
				\item[Grafo Aciclico] $\ra$ un grafo che non contiene nessun ciclo è detto aciclico.
			\end{Descr}
			\item[Componenti Connesse] $\ra$ è una classe di equivalenza dei vertici che verifica la relazione "è raggiungibile da";
			\item[Grafo Fortemente Connesso] $\ra$ un grafo viene detto fortemente connesso se due vertici qualsiasi sono raggiungibili l'uno dall'altro.
		\end{Descr}
		\begin{subParagraph}{Rappresentazione di Grafi}
Un grafo $G=(V,E)$ questo può essere rappresentato in due modi:
			\begin{Descr} 
				\item[Liste di Adiacenza] $\ra$ uso un vettore $adj$ di $\modulo{V}$ liste, una per nodo. Ogni lista $adj[u]$ contiene i nodi raggiungibili dall'$u$-esimo nodo (per i grafi indiretti considero gli archi in entrambe le direzioni). Il peso dell'arco o qualsiasi attributo aggiuntivo viene memorizzato nella lista.
				\begin{Descr} 
					\item[Spazio] $\ra \Theta(V+E)$
					\item[Tempo] $\ra$ elencare tutti i nodi adiacenti ad $u$ costa $\Theta(u.degree)$ mentre determinare se $(u,v)\in E$ costa $O(u.degree)$.
				\end{Descr}
				\item[Matrice di Adiacenza] $\ra$ uso una matrice $\modulo{V}\times\modulo{V}$ dove l'elemento $a_{ij}=\begin{cases}1\qquad\text{se } (i,j)\in E\\ 0\qquad\text{se } (i,j)\notin E\\ \end{cases}$ 
				\begin{Descr} 
					\item[Spazio] $\ra \Theta(V^2)$
					\item[Tempo] $\ra$ elencare tutti i nodi adiacenti ad $u$ costa $\Theta(V)$ mentre determinare se $(u,v)\in E$ costa $O(1)$.
				\end{Descr}
			\end{Descr}
		\end{subParagraph}
		\begin{subParagraph}{Visita in Ampiezza}
Scopre tutti i vertici raggiungibili da un nodo sorgente $s\in V$. Come input ho il grafo $G$ e il nodo sorgente $s$ mentre come output avrò $\forall v\in V$ la distanza $v.d$ da $s$ a $v$ e il predecessore $v.\pi=u$ di $v$ ovvero il penultimo nodo nel cammino da $s$ a $v$. L'insieme degli archi $\{v.\pi,v\} \text{ tale che } v\neq s$ forma un albero. Per la Visita in Ampiezza uso una coda "FIFO" $Q$ con $\proc{Enqueue}$ e $\proc{Dequeue}$. Si può mostrare che i valori di $d$ presenti in $Q$ sono al massimo 2 e se sono due, quelli più piccoli sono stati inseriti prima, quindi saranno i primi ad uscire. Lo pseudocodice è:
			\begin{code}{BFS(G,s)}
\li \FOR{\text{ogni vertice }u\in \attrib{G}{V}-\{s\}\COMMENT{imposto i nodi bianchi}} 
	\li $\attrib{u}{color}\get\text{WHITE}$
	\li $\attrib{u}{d}\get\infty$
	\li $\attrib{u}{\pi}\get\const{nil}$
\END
\li $\attrib{s}{color}\get\text{GRAY}$ \COMMENT{Imposto la sorgente}
\li $\attrib{s}{d}\get0$
\li $\attrib{s}{\pi}\get\const{nil}$
\li $Q\get\emptyset$ \COMMENT{inizializzo la coda FIFO}
\li $\proc{Enqueue}(Q,s)$ 
\li \WHILE{Q\neq\emptyset}
	\li $u\get\proc{Dequeue}(Q)$ \COMMENT{estrae il nodo corrente e lo salva}
	\li \FOR{\text{ogni }v\in \attrib{G}{adj}[u]\COMMENT{esplora i nodi adiacenti  a quello corrente}} 
		\li \IF{\attrib{v}{color}=\text{WHITE}} 
			\li $\attrib{v}{color}\get\text{GRAY}$
			\li $\attrib{v}{d}\get u.d+1$
			\li $\attrib{v}{\pi}\get u$
			\li $\proc{Enqueue}(Q,v)$
		\END
	\END
	\li $\attrib{u}{color}\get \text{BLACK}$ \COMMENT{imposta il nodo corrente come esplorato}
\END
			\end{code}
\bo{Costo}\\
Il tempo di esecuzione complessivo sarà $O(V+E)$ poiché $\proc{Enqueue}$ e $\proc{Dequeue}$ costano $O(1)$, ogni nodo è messo nella coda al massimo una volta e ogni arco viene esplorato al massimo una volta se il grafo è diretto (o al massimo due volte sei il grafo è indiretto). Inoltre si usa $O$ e \underline{non} $\Theta$ perché alcuni nodi potrebbero non essere esplorati.
		\end{subParagraph}
		\begin{subParagraph}{Visita in Profondità}
Esplora sistematicamente ogni arco, anche ri-iniziando da nodi diversi. Come input ho il grafo $G$ mentre come output avrò $\forall v\in V$ il tempo di scoperta $v.d$, il tempo di fine $v.f$ e il predecessore $v.\pi$.\\
I tempi di scoperta e di fine rispettano la relazione $1\leq v.d < v.f \leq 2\modulo{V}$. Lo pseudocodice è:
			\begin{code}{DFS(G)}
\li \FOR{\text{ogni vertice }u\in \attrib{G}{V}}
	\li $\attrib{u}{color}\get \text{WHITE}$
	\li $\attrib{u}{\pi}\get \const{nil}$
\END
\li $time\get0$
\li \FOR{\text{ogni vertice }u\in \attrib{G}{V} \COMMENT{usa tutti i nodi come sorgente}}
	\li \IF{\attrib{u}{color}=\text{WHITE}}
		\li $\proc{DFS-Visit}(G,u)$
	\END
\END
			\end{code} 
			\begin{code}{DFS-Visit(G,u)}
\li $time\get time+1$ \COMMENT{aggiorna il tempo corrente}
\li $\attrib{u}{d}\get time$ \COMMENT{inizia l'esplorazione del nodo corrente}
\li $\attrib{u}{color}\get \text{GRAY}$ 
\li \FOR{\text{ogni }v\in G.adj[u] \COMMENT{esplora tutti i nodi raggiungibili dal corrente}}
	\li \IF{\attrib{v}{color}=\text{WHITE}}
		\li $\attrib{v}{\pi}\get u$
		\li $\proc{DFS-Visit}(G,v)$ 
	\END
\END
\li $\attrib{u}{color}\get \text{BLACK}$ \COMMENT{conclude l'esplorazione del nodo corrente}
\li $time \get time+1$ \COMMENT{aggiorna il tempo corrente}
\li $\attrib{u}{f}\get time$
			\end{code}
\bo{Costo}\\
Esplora ogni nodo e ramo quindi il costo totale sarà $\Theta(V+E)$.
			\begin{Descr} 
	\item[\underline{Teorema delle Parentesi}] (Successori e predecessori) $\ra$ durante una $\proc{DFS}$ di $G=(V,E)$, $\forall$ coppia di vertici $(u,v)$ è soddisfatta solo una delle seguenti condizioni:
				\begin{itemize} 
					\item Gli intervalli $[u.d,u.f]$ e $[v.d,v.f]$ sono completamente disgiunti (cioè non si sovrappongono) allora \bo{nessuno dei due è discendente/predecessore dell'altro} in un albero DF;
					\item L'intervallo di $v$ è completamente contenuto in quello di $u$ allora $\bm{v}$ \bo{è un discendente di } $\bm{u}$ in un albero DF;
					\item L'intervallo di $u$ è completamente contenuto in quello di $v$ allora $\bm{u}$ \bo{è un discendente di } $\bm{v}$ in un albero DF.
				\end{itemize} 
				\begin{Descr} 
					\item[Corollario:] $v$ è un discendente di $u$ nella foresta DF \SSE \begin{tcenter}$u.d<v.d<v.f<u.f$.\end{tcenter}
				\end{Descr}
				\item[Foresta DF] $\ra$ Quando scopro un vertice $v$ durante l'ispezione di $adj[u]$ ho $v.\pi=u$. Il sottografo con radice $u$ forma una foresta  DF contenente almeno un albero DF. Quando esploro l'arco $(u,v)$ di un albero DF, il nodo $u$ è grigio e il nodo $v$ è bianco.
				\item[\underline{Teorema del Cammino Bianco}] $\ra$ In una forsesta DF di un grafo $G=(V,E)$ il vertice $v$ è un discendente di $u$ \SSE al tempo $u.d$ esiste un cammino bianco da $u$ a $v$ in $G$ fatto solo di nodi bianchi (eccetto $u$ che è appena diventato grigio). 
				\item[Classificazione degli Archi] $\ra$ Durante la $\proc{DFS}$ posso classificare gli archi del grafo:
				\begin{Descr} 
					\item[T] $\ra$ se il ramo $(u,v)$ è un ramo della foresta DF;
					\item[F] $\ra$ se $v$ è un discendente di $u$ ma non fa parte della foresta DF;
					\item[B] $\ra$ se $u$ è un discendente di $v$;
					\item[C] $\ra$ qualsiasi altro arco tra i nodi dello stesso albero o di alberi diversi.
				\end{Descr}
Posso anche classificare gli archi durante la $\proc{DFS}$ utilizzando i colori dei vertici. Durante la visita del ramo $(u,v)$ se $v.color$ è:
				\begin{Descr} 
					\item[WHITE] $\ra$ il ramo è T;
					\item[GRAY] $\ra$ il ramo è B;
					\item[BLACK] $\ra$ il ramo è F o C:
					\begin{Descr} 
						\item[$u.d<v.d$]: il ramo è F ($v$ è un discendente di $u$);
						\item[$u.d>v.d$]: il ramo è C.
					\end{Descr}
				\end{Descr}
				\item[\underline{Teorema}] $\ra$ Se il grafo $G=(V,E)$ è indiretto $\implies$ la foresta DF ho solo archi T e B.
				\begin{Descr} 
					\item[LEMMA:] un grafo diretto è \bo{aciclico} \SSE una visita $\proc{DFS}$ di $G$ non genera archi B.
				\end{Descr}
			\end{Descr}
		\end{subParagraph}
		\begin{subParagraph}{Ordinamento Topologico}
L'ordinamento topologico di un grafo aciclico (sia diretto che indiretto) $G=(V,E)$ è un ordinamento lineare di tutti i suoi vertici tale che se $(u,v)\in E$ allora da qualche parte nell'ordinamento $u$ appare prima di $v$ (è diverso dall'ordinamento di un insieme di numeri). Non è possibile effettuare un ordinamento lineare su un grafo ciclico. E' utile per gestire oggetti che hanno un ordine parziale, ad esempio:
			\begin{tcenter}
$a>b$ e $b>c \implies a>c$. 
			\end{tcenter}
Ma potrei anche non sapere con precisione l'ordinamento totale:
			\begin{tcenter}
$a>c$ e $b>c\implies a>b$ o $a<b$. 
			\end{tcenter}
E' comunque possibile trovare un ordinamento totale anche da un ordinamento parziale, usando la $\proc{DFS}$. Lo pseudocodice è:
			\begin{code}{TopologicalSort(G)}
\li Chiama $\proc{DFS(G)}$ per calcolare i tempi di completamento \\ $v.f$ per ogni vertice $v$
\li Completata l'ispezione di ogni vertice, lo inserisce in testa \\ ad una lista concatenata
\li \RETURN lista concatenata di vertici
			\end{code}
\bo{Costo}\\
Il tempo complessivo è $\Theta(V+E)$.\\
\bo{Correttezza}\\
Mostriamo che se $(u,v)\in E$ allora $v.f<u.f$. Quando esploro $(u,v)$ di che colore è e $v$?
			\begin{descr}{1}
				\item[Grigio] $\ra$ impossibile altrimenti $(u,v)$ sarebbe un arco B(back) $\implies$ contraddizione con ipotesi (no grafi ciclici);
				\item[Bianco] $\ra$ allora $v$ diventa discendente di $u$ $\implies u.d<v.d<v.f<u.f$ (per th. Parentesi) $\implies v.f<u.f$;
				\item[Nero] $\ra$ allora $v$ è terminato ma $u$ ancora no $\implies v.f<u.f$
 			\end{descr} 
		\end{subParagraph}
		\begin{subParagraph}{Componenti Fortemente Connesse}
Dato il grafo diretto \G una componente fortemente connessa (SCC) di $G$ è un insieme massimale di vertici $C\subseteq V$ tale che per ogni $u,v\in C$ esistono entrambi i cammini "$u\to v$" e "$v\to u$". Uso la matrice di adiacenza $G$ e la sua trasposta $G^\proc{t}$ per ottenere singolarmente le componenti connesse del grafo \G.
			\begin{Descr} 
				\item[Trasposta di $G$] $\ra$ $G^\proc{t}=(V,E^\proc{t})$, ovvero inverto la direzione di tutti gli archi del grafo. Se rappresento il grafo con la matrice di adiacenza il tempo è lineare altrimenti, se lo faccio con le liste di adiacenza il costo diventa $\Theta(V+E)$. Inoltre $G$ e $G^\proc{t}$ hanno le stesse SCC.\\
Definisco anche il grafo delle componenti fortemente connesse $G^{SCC}=(V^{SCC},E^{SCC})$ che ha un vertice per ogni SCC di $G$ e un arco solo se c'è un arco che connette le SCC in $G$.  
				\item [Lemma] $G^{SCC}$ è un DAG (grafo diretto aciclico).
				%TODO aggiungere dimostrazione ...
			\end{Descr}
L'algoritmo che restituisce il $G^{SCC}$ è il seguente:
			\begin{code}{Strongly-Connected-Components(G)}
\li Chiama $\proc{DFS}(G)$ \COMMENT{per calcolare $v.f$ di ogni vertice $v$}
\li Calcola $G^\proc{t}$ 
\li Chiama $\proc{DFS}(G^\proc{t})$ \COMMENT{i tempi $u.f$ sono considerati in ord. decresc.}
\li Genera come output le radici di ogni SCC generata da\\ $\proc{DFS}(G^\proc{t})$ come foresta DF. \COMMENT{In pratica crea un grafo $G^{SCC}$}
			\end{code}
		\end{subParagraph}
\bo{Costo}
Il tempo complessivo è $\Theta(V+E)$.
%TODO Aggiungere Dimostrazione di Correttezza ...
		\end{myParagraph}

		\begin{myParagraph}{STRUTTURE DATI PER INSIEMI DISGIUNTI}
Questa pratica è anche nota come UnionFind. Serve a gestire una collezione $S={S_1,...,S_k}$ di \bo{insiemi disgiunti dinamici}. Ogni insieme è rappresentato da un \bo{rappresentante}, non è rilevante quale sia, ma devo necessariamente essere sempre lo stesso. Le operazioni che implementano questa tecnica sono:
			\begin{Descr} 
		\item[$\bm{\proc{Make-Set}(x)}$] $\ra$ crea un nuovo insieme $S_i={x}$ e lo aggiunge ad $S$;
		\item[$\bm{\proc{Union}(x,y)}$] $\ra$ elimina gli insiemi disgiunti di $x,y$ da $S$ e ci aggiunge la loro unione: $S\get S-S_x-S_y+(S_x\bigcup S_y)$;
		\item[$\bm{\proc{Find-Set}(x)}$] $\ra$ ritorna il rappresentante dell'insieme che contiene $x$.
			\end{Descr}
			\begin{subParagraph}{Componenti Connesse}
In un grafo \G i vertici $u$ e $v$ sono nella stessa componente connessa \SSE c'è un cammino tra di loro. La componente connessa è una classe di equivalenza che risponde alla domanda "E' raggiungibile da?". Un grafo \bo{indiretto} può essere connesso, mentre un grafo \bo{diretto} può essere fortemente connesso. L'algoritmo che trova le componenti connesse é:
				\begin{code}{Connected-Components(G)}
\li \FOR{\text{ogni vertice }v\in G.V}
	\li $\proc{Make-Set}(v)$
\END
\li \FOR{\text{ogni arco }(u,v)\in G.E}
	\li \IF{\proc{Find-Set}(u)\neq\proc{Find-Set}(v)}
		\li $\proc{Union}(u,v)$
	\END
\END
				\end{code}
Gli insiemi sono implementati con le \textbf{liste concatenate}:\\
ogni insieme è rappresentato come una lista concatenata diversa e ogni elemento della lista contiene elemento, puntatore al rappresentante e puntatore all'elemento successivo.
\bo{Operazioni:}
\begin{descr}{1}
		\item[$\bm{\proc{Make-Set}(x)}$] $\ra$ crea una lista con un solo elemento;
		\item[$\bm{\proc{Union}(x,y)}$] $\ra$ è la parte costosa dell'algoritmo (accodare una lista lunga ad una lista corta può richiedere molte operazioni)
		\begin{Descr} 
			\item[Soluzione:] \underline{Unione Pesata} $\ra$ tra la due liste da unire si accoda sempre la più corta alla più lunga.
		\end{Descr}
		\item[$\bm{\proc{Find-Set}(x)}$] $\ra$ ritorna il puntatore al rappresentante.
\end{descr}
\bo{Costo}\\
Utilizzando l'Unione Pesata per una sequenza di $m$ operazioni su $n$ elementi il costo è $O(m+n\log_2 n)$
			\end{subParagraph}
			\begin{subParagraph}{Heap}
Un heap è un'albero binario quasi completo. l'altezza di un nodo è il numero di archi nel cammino più semplice dal nodo ad una foglia. L'altezza dell'heap, che è uguale all'altezza della radice è $\Theta(\log_2 n)$. L'heap può essere implementato usando un'array:
				\begin{descr}{1}
					\item[Radice dell'albero]: è il primo elemento cioè $A[1]$;
					\item[Padre dell'i-esimo elemento]: il padre di $A[i]$ è $A[i/2]$; 
					\item[Figlio sx dell'i-esimo elemento]: il figlio sx di $A[i]$ è $A[2\cdot i]$
					\item[Figlio sx dell'i-esimo elemento]: il figlio dx di $A[i]$ è $A[2\cdot i+1]$
				\end{descr}		
Esistono due tipi di heap:
				\begin{descr}{0}
					\item[max-heap] $\ra\forall$ nodo $i$, eccetto la radice $A[\proc{Parent}(i)]\geq A[i]$ 
					\item[min-heap] $\ra\forall$ nodo $i$, eccetto la radice $A[\proc{Parent}(i)]\leq A[i]$ 
				\end{descr}
Da qui in poi useremo sempre il \bo{max-heap} per comodità, infatti il nodo con chiave massima è sempre la radice.\\
Serve un algoritmo che conservi le proprietà del max-heap, ovvero che aggiusti l'heap dopo l'inserimento del nodo $i$. Lo pseudocodice è:
				\begin{code}{Max-Heapify(A,i)}
\li $l\get\proc{left}(i)$
\li $r\get\proc{right}(i)$
\li \IF{l\leq A.heap-size \text{ AND } A[l]>A[i]}
	\li $massimo\get l$
\li \ELSE
	\li $massimo\get i$
\END
\li \IF{r\leq A.heap-size \text{ AND } A[r]>A[massimo]}
	\li $massimo\get r$
\END
\li \IF{massimo\neq i}
	\li \bo{scambia} $A[i] \sse A[massimo]$
	\li $\proc{Max-Heapify}(A,massimo)$
\END
				\end{code}
\bo{Costo}\\
L'heap è alto $O(\log_2 n)$ poiché è un albero quasi completo. Il costo ad ogni livello è costante (3 confronti e max 2 scambi)$\implies$ il costo complessivo è intuitivamente $O(\log_2 n)$. \\
\bo{Correttezza}\\ 
Si può ottenere con l'invariante di ciclo (perché è ricorsivo in coda).
				\begin{descr}{1}
					\item[\underline{Costruire un heap}:] Dato un array non ordinato, produce un max-heap bottom-up. Lo pseudocodice è: 
				\end{descr}
				\begin{code}{BuildMaxHeap(A)}
\li $\attrib{A}{heap-size}\get \attrib{A}{length}$
\li \FOR{i \get \floor{\attrib{A}{lenght}/2} \DOWNTO 1}
	\li $\proc{Max-Heapify}(A,i)$
\END
				\end{code}
			\begin{subParagraph}{Code con Priorità}
Posso implementare una coda con priorità usando un max-heap. Ogni elemento dell'insieme dinamico $S$ ha una chiave che indica la priorità dell'elemento. Implementando una coda con un max-heap si ottiene un compromesso tra \underline{coda} (inserimento veloce, estrazione lenta) e \underline{coda ordinata} (inserimento lento ed estrazione veloce). Con il max-heap le operazioni di inserimento ed estrazione prendono entrambe $O(\log_2 n)$. Le operazioni di gestione della coda con priorità sono le seguenti:
				\begin{code}{Heap-Maximum(A)}
\li \RETURN $A[1]$
				\end{code}
\bo{Costo}\\
Il tempo totale è $\Theta(1)$\\
				\begin{code}{Heap-Extract-Max(A)}
\li \IF{\attrib{A}{heap-size}<1}
	\li \Error "underflow dell'heap"
\END
\li $max\get A[1]$
\li $A[1]\get A[\attrib{A}{heap-size}]$
\li $\attrib{A}{heap-size}\get \attrib{A}{heap-size}-1$
\li $\proc{Max-Heapify}(A,1)$
\li \RETURN $max$
				\end{code}
			\end{subParagraph}
\bo{Costo}\\
Il tempo totale è $O(\log_2 n)$\\
				\begin{code}{Heap-Increase-Key(A,i,key)}
\li \IF{key<A[i]}
	\li \Error "la nuova chiave è più piccola della corrente"
\END
\li $A[i]\get key$
\li \WHILE{i>1 \text{ AND } A[\proc{Parent}(i)]<A[i]}
	\li \bo{scambia } $A[i] \sse A[\proc{Parent}(i)]$
	\li $i\get \proc{Parent}(i)$
\END
				\end{code}
\bo{Costo}\\
Il tempo totale è $O(\log_2 n)$\\
				\begin{code}{Max-Heap-Insert(A,key)}
\li $\attrib{A}{heap-size}\get \attrib{A}{heap-size}+1$
\li $A[\attrib{A}{heap-size}]\get -\infty$
\li $\proc{Heap-Increase-Key}(A,\attrib{A}{heap-size},key)$
				\end{code}
\bo{Costo}\\
Il tempo totale è $O(\log_2 n)$\\
			\end{subParagraph}
	\end{myParagraph}
	
	\begin{myParagraphEnd}{Alberi di Connessione Minimi}
Dato un grafo non diretto \G con $V$ nodi ed $E$ archi ognuno avente il peso $w$. Quello che vogliamo è trovare un albero di connessione $T$ che connette tutti i vertici. Un albero di connessione minimo invece connette tutti i nodi con gli archi con il minimo peso (MST - Minimum Spanning Tree).
		\begin{Descr} 
			\item[Proprità MST] $\ra$ un MST ha $\modulo{V}-1$ archi ed è aciclico. Inoltre può esistere più di un MST per lo stesso albero.
		\end{Descr}
		\begin{subParagraph}{Generic MST}
Per questo problema, è conveniente prima cercare un'invariante di ciclo e poi costruire un algoritmo. Prendiamo un insieme di archi $A$ che all'inizio vuoto ($A=\emptyset$). 
			\begin{Descr} 
				\item[I.C.] $\ra$ "ad ogni iterazione aggiungo un arco ad $A$ solo se $A$ rimane un sottoinsieme di qualche MST".
			\end{Descr}
Se $A\subseteq \text{MST}$ all'ultima iterazione abbiamo aggiunto un \bo{arco sicuro}, cioè che fa sicuramente parte di un qualche MST. Matematicamente: $(u,v) \text{ è sicuro per } A \sse (A\cup \{(u,v)\})\subseteq$ MST. Lo pseudocodice è:
			\begin{code}{GenericMST(G,w)}
\li $A\get\emptyset$
\li \WHILE{A \text{non forma un albero di connessione}}
	\li trova un arco $(u,v)$ sicuro per $A$
	\li $A \get A\cup(u,v)$
\END
			\end{code}
			\begin{Descr} 
				\item[Trovare un arco sicuro]: $\ra$ Dato \G, l'arco con il peso minore $(c,f)$ è un arco sicuro per $G$ \SSE non c'è un altro cammino che connette $f$ e $f$, cioè:\\
Sia $S\subset V$ un qualunque insieme di vertici tale che $c\in S$ e $f\in V-S$. Ogni MST deve avere almeno un arco che connetta $S$ con $V-S$, voglio scegliere l'arco che li connette con il peso minore.
				\item[Definizioni]:\\
				\begin{Descr} 
					\item[taglio] $\ra$ Dato un grafo \G non orientato e definisco $S\subset V$ e $A\subseteq E$. Allora un taglio $(S,V-S)$ è una partizione dei vertici $V$ in due insiemi disgiunti $S$ e $V-S$. Un arco $(u,v)$ \bo{attraversa} un taglio \SSE $u\in S$ e $v\in V-S$ (o viceversa). Infine un taglio \bo{rispetta} un insieme di archi $A$ \SSE nessun arco attraversa il taglio. 
					\item[arco leggero] $\ra$ un arco è leggero per un taglio \SSE ha il peso minimo tra tutti gli archi che attraversano il taglio.
				\end{Descr}
				\item[Teorema]: Dato \G, sia $A$ un sottoinsieme di un MST di $G$, sia $(S,V-S)$ un taglio che rispetta l'insieme di archi $A$ e sia $(u,v)$ un arco leggero di che attraversa il taglio $(S,V-S)\implies (u,v)$ è un \bo{arco sicuro} per $A$.
				\item[Dimostrazione]: Sia $T$ un qualsiasi MST che contiene $A$ allora se:
				\begin{Descr} 
					\item[\bm{$(u,v)\subseteq T$}] $\implies$ $(u,v)$ è sicuro per $A$; 
					\item[\bm{$(u,v)\not\subseteq T$}] $\implies$ costruisco $T'$ un altro MST tale che $A\cup (u,v)\subseteq T'$.\\	
					Infatti poiché $T$ è un MST, ha un cammino unico, detto $p$ tra $u$ e $v$. \\
					Supponendo che esista un altro arco $(x,y)\subseteq T$ oltre a $(u,v)$ che attraversa il taglio $(S,V-S)$. \\
					Allora se $(u,v)$ è leggero significa che i pesi degli archi sono $w(u,v)\leq w(x,y)$. \\
					Quindi $T'$ avrà come peso $w(T')=w(T)-w(x,y)+w(u,v)\leq w(T)$ poiché $w(u,v)\leq w(x,y)$. \\
					Infine poiché $T'$ è un albero di copertura e $T$ è un MST con $w(T)\geq w(T')$ allora anche $T'$ sarà un MST.
					$\implies (u,v)$ è sicuro per $A$.
				\end{Descr} 
				\item[Corollario]: Sia $G_A=(V,A)$ la foresta che contiene le singole componenti connesse (all'inizio un albero per ogni nodo), sia $C=(V_C,E_C)$ una componente connessa di $G_A$, allora se $(u,v)$ è un arco leggero che connette $C$ a una qualsiasi altra componente in $G_A\implies(u,v)$ è un arco sicuro per $A$.
				\item[Dimostrazione]: (uguale a quella precedente, basta porre $S=V_C$).
				%TODO dimostrazione uguale a quella precedente ma con S=V_C
			\end{Descr}
In $\proc{GenericMST}$, iterando $\modulo{V}-1$ volte, alla fine ottengo $\modulo{V}-1$ archi sicuri per $A$ ovvero un MST. Questa affermazione e il corollario ci portano alla formulazione di Kruskal.
		\end{subParagraph}
		\begin{subParagraph}{MST Kruskal}
Uso le operazioni per le strutture dati per gli insiemi disgiunti. Quindi chiamo $\proc{MakeSet}$ per ogni nodo e chiamo $\proc{Union}$ ogni volta che trovo un arco sicuro. Lo pseudocodice è:
			\begin{code}{MST-Kruskal(G,w)}
\li $A\get\emptyset$
\li \FOR{\text{ogni vertice }v\in\attrib{G}{V}}
	\li $\proc{MakeSet}(v)$
\END
\li \bo{ordina} gli archi $\attrib{G}{E}$ in ordine di peso non decrescente
\li \FOR{\text{ogni arco }(u,v)\in \attrib{G}{E} \text{ in ordine di peso non decresc.}}
	\li \IF{\proc{FindSet}(u)\neq \proc{FindSet}(v)}
		\li $A\get A\cup\{(u,v)\}$
		\li $\proc{Union}(u,v)$
	\END
\END
\li \RETURN $A$
			\end{code}
\bo{Costo}\\
Il tempo di esecuzione dipende dall'implementazione di union-find.
		\end{subParagraph}
		\begin{subParagraph}{MST Prim}
L'algoritmo di Prim è un caso particolare del $\proc{GenericMST}$. $A$ è sempre un albero, $r$ è una radice arbitraria. Ad ogni passo trova un arco leggero che attraversa il taglio $(V_A, V-V_A)$, dove $V_A$ è l'insieme dei vertici toccati da archi di $A$, e lo aggiunge ad $A$. Lo pseudocodice è:
			\begin{code}{Prim-MST(G,w,r)}
\li $Q\get\emptyset$ \COMMENT{uso coda priorità implementata con min-heap}
\li \FOR{\text{ogni }u\in \attrib{G}{V}}
	\li $\attrib{u}{key}\get \infty$
	\li $\attrib{u}{\pi}\get\const{nil}$
	\li $\proc{Insert}(Q,u)$
\END
\li $\proc{Decrease-Key}(Q,r,0)$ \COMMENT{imposta la chiave della radice a 0}
\li \WHILE{Q\neq\emptyset}
	\li $u\get\proc{Extract-Min}(Q)$
	\li \FOR{\text{ogni }v\in\attrib{G}{adj[u]}}
		\li \IF{v\in Q \text{ AND } w(u,v)<\attrib{v}{key}}
			\li $\attrib{v}{\pi}\get u$
			\li $\proc{Decrease-Key}(Q,v,w(u,v))$
		\END
	\END
\END
			\end{code}
\bo{I.C.}\\
"Prima di ogni iterazione del $while$:
\begin{descr} {1}
	\item[1)] $A={(v,\attrib{v}{\pi}) \text{ tale che } v\in V-\{r\}-Q}$;
	\item[2)] in vertici inseriti in MST sono quelli in $V-Q$;
	\item[3)] $\forall v\in Q$ se $\attrib{v}{\pi}\neq \const{nil}$ allora $\attrib{v}{key}<\infty$ ed è il peso dell'arco leggero $(v,\attrib{v}{\pi})$ che collega $v$ ad un qualche vertice in $A$."
\end{descr}
Alla fine dell'esecuzione avrò che $V_A=V\implies Q=\emptyset \implies A$ è un MST.
		\end{subParagraph}
	
	
	
	
	
	
	
	
	
	
	
	
	\end{myParagraphEnd}
\end{formulario}
\end{document}
